# LLM Usage Guide

## Overview
This skill provides comprehensive guidelines for using Large Language Models (LLMs) in the BlockMe tax knowledge base system. It covers multi-provider configuration, prompt engineering, cost optimization, and integration patterns for document processing and tax Q&A.

## LLM Provider Configuration

### 1. Multi-Provider Strategy
BlockMe uses a multi-provider approach for reliability and cost optimization:

```python
# ✅ GOOD - Multi-provider configuration with fallback
# app/llm/providers.py
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
from app.config import LLMProvider, LLM_MODELS
from app.errors import LLMServiceError

class BaseLLMProvider(ABC):
    """Base class for all LLM providers."""

    def __init__(self, provider: LLMProvider):
        self.provider = provider
        self.models = LLM_MODELS[provider.value]
        self.rate_limiter = RateLimiter(
            requests_per_minute=60,
            requests_per_hour=1000
        )

    @abstractmethod
    async def analyze_document(self, content: str, document_type: str) -> Dict[str, Any]:
        """Analyze document content."""
        pass

    @abstractmethod
    async def answer_tax_question(self, question: str, context: list[str]) -> str:
        """Answer tax-related questions."""
        pass

    @abstractmethod
    async def extract_entities(self, text: str) -> Dict[str, Any]:
        """Extract tax entities from text."""
        pass

class OpenAIProvider(BaseLLMProvider):
    """OpenAI GPT provider implementation."""

    def __init__(self):
        super().__init__(LLMProvider.OPENAI)
        self.client = AsyncOpenAI(api_key=env_config.openai_api_key)

    async def analyze_document(self, content: str, document_type: str) -> Dict[str, Any]:
        """Analyze tax document using GPT-4 with vision capabilities."""

        await self.rate_limiter.acquire()

        prompt = self._build_document_analysis_prompt(content, document_type)

        try:
            response = await self.client.chat.completions.create(
                model=self.models["vision"],
                messages=[
                    {"role": "system", "content": prompt["system"]},
                    {"role": "user", "content": prompt["user"]}
                ],
                temperature=0.1,  # Low temperature for consistent analysis
                max_tokens=2000,
                response_format={"type": "json_object"}
            )

            return {
                "analysis": json.loads(response.choices[0].message.content),
                "model_used": self.models["vision"],
                "provider": "openai",
                "tokens_used": response.usage.total_tokens
            }

        except OpenAIError as e:
            if e.status == 429:
                raise LLMQuotaExceededError("OpenAI quota exceeded")
            elif e.status >= 500:
                raise LLMServiceError("OpenAI service temporarily unavailable")
            else:
                raise LLMServiceError(f"OpenAI API error: {e.message}")

class AnthropicProvider(BaseLLMProvider):
    """Anthropic Claude provider implementation."""

    def __init__(self):
        super().__init__(LLMProvider.ANTHROPIC)
        self.client = AsyncAnthropic(api_key=env_config.anthropic_api_key)

    async def analyze_document(self, content: str, document_type: str) -> Dict[str, Any]:
        """Analyze tax document using Claude."""

        await self.rate_limiter.acquire()

        prompt = self._build_document_analysis_prompt(content, document_type)

        try:
            response = await self.client.messages.create(
                model=self.models["vision"],
                max_tokens=2000,
                temperature=0.1,
                system=prompt["system"],
                messages=[
                    {"role": "user", "content": prompt["user"]}
                ]
            )

            # Claude doesn't enforce JSON format, so we need to parse
            content_text = response.content[0].text
            analysis = self._extract_json_from_response(content_text)

            return {
                "analysis": analysis,
                "model_used": self.models["vision"],
                "provider": "anthropic",
                "tokens_used": response.usage.input_tokens + response.usage.output_tokens
            }

        except AnthropicError as e:
            if e.status == 429:
                raise LLMQuotaExceededError("Anthropic quota exceeded")
            elif e.status >= 500:
                raise LLMServiceError("Anthropic service temporarily unavailable")
            else:
                raise LLMServiceError(f"Anthropic API error: {e.message}")

class GLMProvider(BaseLLMProvider):
    """GLM (Zhipu AI) provider implementation."""

    def __init__(self):
        super().__init__(LLMProvider.GLM)
        self.client = AsyncOpenAI(
            api_key=env_config.glm_api_key,
            base_url="https://open.bigmodel.cn/api/coding/paas/v4/"
        )

    async def analyze_document(self, content: str, document_type: str) -> Dict[str, Any]:
        """Analyze tax document using GLM-4."""

        await self.rate_limiter.acquire()

        prompt = self._build_document_analysis_prompt(content, document_type)

        try:
            response = await self.client.chat.completions.create(
                model=self.models["vision"],
                messages=[
                    {"role": "system", "content": prompt["system"]},
                    {"role": "user", "content": prompt["user"]}
                ],
                temperature=0.1,
                max_tokens=2000
            )

            content_text = response.choices[0].message.content
            analysis = self._extract_json_from_response(content_text)

            return {
                "analysis": analysis,
                "model_used": self.models["vision"],
                "provider": "glm",
                "tokens_used": response.usage.total_tokens
            }

        except Exception as e:
            raise LLMServiceError(f"GLM API error: {str(e)}")
```

### 2. Provider Selection Strategy
```python
# ✅ GOOD - Intelligent provider selection
# app/llm/provider_manager.py

class ProviderManager:
    """Manages LLM provider selection and fallback."""

    def __init__(self):
        self.providers = {
            LLMProvider.OPENAI: OpenAIProvider(),
            LLMProvider.ANTHROPIC: AnthropicProvider(),
            LLMProvider.GLM: GLMProvider()
        }

        self.primary_provider = LLMProvider.OPENAI  # Default
        self.fallback_providers = [LLMProvider.ANTHROPIC, LLMProvider.GLM]

        # Provider capabilities mapping
        self.capabilities = {
            "vision_analysis": [LLMProvider.OPENAI, LLMProvider.ANTHROPIC],
            "tax_qa": [LLMProvider.OPENAI, LLMProvider.ANTHROPIC, LLMProvider.GLM],
            "entity_extraction": [LLMProvider.OPENAI, LLMProvider.GLM],
            "cost_optimized": [LLMProvider.GLM]  # Cheapest for simple tasks
        }

    def select_provider(
        self,
        task_type: str,
        user_preference: Optional[LLMProvider] = None,
        cost_sensitivity: float = 0.5  # 0 = performance priority, 1 = cost priority
    ) -> LLMProvider:
        """Select optimal provider based on task and constraints."""

        # User preference takes priority
        if user_preference and self._is_provider_available(user_preference):
            return user_preference

        # Cost-sensitive selection
        if cost_sensitivity > 0.7 and task_type == "tax_qa":
            return LLMProvider.GLM  # Use cheapest provider for cost-sensitive users

        # Task-specific capability matching
        capable_providers = self.capabilities.get(task_type, self.providers.keys())

        # Check provider health and availability
        for provider in capable_providers:
            if self._is_provider_available(provider):
                return provider

        # Fallback to any available provider
        for provider in self.providers.keys():
            if self._is_provider_available(provider):
                return provider

        raise LLMServiceError("No LLM providers are currently available")

    def _is_provider_available(self, provider: LLMProvider) -> bool:
        """Check if provider is healthy and within quota."""
        provider_instance = self.providers[provider]
        return (
            not provider_instance.circuit_breaker.is_open() and
            provider_instance.rate_limiter.can_acquire() and
            not provider_instance.quota_exceeded
        )
```

## Prompt Engineering Patterns

### 1. Template System with Dynamic Fragments
```python
# ✅ GOOD - Modular prompt template system
# app/llm/prompts/templates.py

from enum import Enum
from typing import Dict, List, Any
from dataclasses import dataclass

class PromptTemplate(Enum):
    DOCUMENT_ANALYSIS = "document_analysis"
    TAX_QA = "tax_qa"
    ENTITY_EXTRACTION = "entity_extraction"
    TAX_CALCULATION = "tax_calculation"

class PromptFragment(Enum):
    # Document analysis fragments
    BASIC_INSTRUCTIONS = "basic_instructions"
    TAX_RULES_CANADA = "tax_rules_canada"
    TAX_RULES_PROVINCE = "tax_rules_province"
    JSON_FORMAT_SPEC = "json_format_spec"
    CONFIDENCE_SCORING = "confidence_scoring"

    # Tax QA fragments
    QA_CONTEXT = "qa_context"
    CITATION_REQUIREMENTS = "citation_requirements"
    DISCLAIMER_TEMPLATE = "disclaimer_template"

@dataclass
class PromptContext:
    """Context for prompt generation."""
    jurisdiction: str = "on"  # Default Ontario
    tax_year: int = 2024
    document_type: str = "general"
    user_expertise: str = "general"  # beginner, intermediate, expert
    language: str = "english"

class PromptBuilder:
    """Builds prompts from templates and fragments."""

    def __init__(self):
        self.templates = self._load_templates()
        self.fragments = self._load_fragments()

    def build_prompt(
        self,
        template: PromptTemplate,
        context: PromptContext,
        dynamic_data: Dict[str, Any]
    ) -> Dict[str, str]:
        """Build complete prompt from template and fragments."""

        template_content = self.templates[template.value]

        # Select fragments based on context
        selected_fragments = self._select_fragments(template, context)

        # Combine template with fragments
        system_prompt = self._assemble_prompt(
            template_content,
            selected_fragments,
            context,
            dynamic_data
        )

        # Generate user prompt
        user_prompt = self._generate_user_prompt(template, dynamic_data)

        return {
            "system": system_prompt,
            "user": user_prompt,
            "metadata": {
                "template": template.value,
                "fragments": [f.value for f in selected_fragments],
                "context": context.__dict__
            }
        }

    def _select_fragments(self, template: PromptTemplate, context: PromptContext) -> List[PromptFragment]:
        """Select relevant fragments based on template and context."""

        if template == PromptTemplate.DOCUMENT_ANALYSIS:
            fragments = [
                PromptFragment.BASIC_INSTRUCTIONS,
                PromptFragment.TAX_RULES_CANADA,
                PromptFragment.JSON_FORMAT_SPEC,
                PromptFragment.CONFIDENCE_SCORING
            ]

            # Add province-specific rules if not federal
            if context.jurisdiction != "federal":
                fragments.append(PromptFragment.TAX_RULES_PROVINCE)

            return fragments

        elif template == PromptTemplate.TAX_QA:
            return [
                PromptFragment.QA_CONTEXT,
                PromptFragment.CITATION_REQUIREMENTS,
                PromptFragment.DISCLAIMER_TEMPLATE
            ]

        return []

    def _assemble_prompt(
        self,
        template: str,
        fragments: List[PromptFragment],
        context: PromptContext,
        dynamic_data: Dict[str, Any]
    ) -> str:
        """Assemble final prompt by combining template and fragments."""

        fragment_content = "\n\n".join(
            self.fragments[fragment.value] for fragment in fragments
        )

        # Substitute context variables
        prompt = template.format(
            fragments=fragment_content,
            jurisdiction=context.jurisdiction.upper(),
            tax_year=context.tax_year,
            **dynamic_data
        )

        return prompt

# Template definitions
DOCUMENT_ANALYSIS_TEMPLATE = """
You are a Canadian tax expert analyzing documents for {jurisdiction} jurisdiction in {tax_year}.

{fragments}

Your task is to analyze the provided document and extract tax-relevant information.
"""

TAX_QA_TEMPLATE = """
You are a Canadian tax expert providing advice for {jurisdiction} jurisdiction in {tax_year}.

{fragments}

Answer the user's tax question based on the provided context.
"""

# Fragment definitions
BASIC_INSTRUCTIONS_FRAGMENT = """
ANALYSIS INSTRUCTIONS:
1. Carefully read the entire document
2. Identify all tax-relevant information
3. Extract specific numerical values
4. Note any uncertainties or ambiguities
5. Provide confidence scores for each extracted value

ACCURACY REQUIREMENTS:
- Only extract information that is clearly stated
- Use null for missing information
- Provide confidence scores from 0.0 to 1.0
- Flag any potential inconsistencies
"""

TAX_RULES_CANADA_FRAGMENT = """
CANADIAN TAX CONTEXT:
- Federal tax rate: 15% on taxable income
- Provincial tax rates vary by province
- GST/HST applies to most goods and services
- RRSP contributions are tax-deductible
- T-slips are used for reporting various income types
"""

JSON_FORMAT_SPEC_FRAGMENT = """
RESPONSE FORMAT:
Provide your analysis in this exact JSON format:
{{
    "document_type": "tax_return|invoice|receipt|contract|other",
    "tax_year": <year>,
    "jurisdiction": "{jurisdiction_lower}",
    "extracted_data": {{
        "total_income": <amount or null>,
        "tax_paid": <amount or null>,
        "business_expenses": <amount or null>,
        "deductions": {{
            "rrsp_contributions": <amount or null>,
            "business_expenses": <amount or null>,
            "other": <amount or null>
        }}
    }},
    "confidence_scores": {{
        "overall": <0.0 to 1.0>,
        "total_income": <0.0 to 1.0>,
        "tax_paid": <0.0 to 1.0>
    }},
    "notes": "<additional observations or concerns>",
    "missing_information": "<what couldn't be extracted>"
}}
"""
```

### 2. Document Analysis Prompts
```python
# ✅ GOOD - Specialized document analysis prompts
# app/llm/prompts/document_analysis.py

class DocumentAnalysisPrompts:
    """Specialized prompts for different document types."""

    @staticmethod
    def build_tax_return_prompt(document_content: str, context: PromptContext) -> str:
        """Build prompt for T1 General tax return analysis."""

        return f"""
        Analyze this Canadian T1 General tax return for {context.jurisdiction.upper()} province in {context.tax_year}.

        DOCUMENT CONTENT:
        {document_content}

        FOCUS AREAS:
        1. Total income reported (line 15000)
        2. Net income (line 23600)
        3. Taxable income (line 26000)
        4. Federal tax payable (line 42000)
        5. Provincial/territorial tax (line varies by province)
        6. Total tax payable (line 43500)
        7. Total tax paid (line 48500)
        8. Refund or balance owing (line 48400)

        SPECIAL ATTENTION:
        - Verify mathematical calculations
        - Cross-check totals and subtotals
        - Note any unusual values or patterns
        - Identify any errors or inconsistencies

        Provide your analysis in the specified JSON format with confidence scores.
        """

    @staticmethod
    def build_invoice_prompt(document_content: str, context: PromptContext) -> str:
        """Build prompt for invoice analysis."""

        return f"""
        Analyze this business invoice for tax purposes in {context.jurisdiction.upper()}.

        DOCUMENT CONTENT:
        {document_content}

        BUSINESS TAX RELEVANCE:
        1. GST/HST amount and rate
        2. Business expense category
        3. Date and supplier information
        4. Description of goods/services
        5. Total amount before tax
        6. Business use percentage (if applicable)

        GST/HST CONSIDERATIONS:
        - Verify GST/HST registration number of supplier
        - Check correct GST/HST rate application
        - Identify input tax credits (ITC) eligibility
        - Note any provincial sales tax (PST) if applicable

        DEDUCTIBILITY ANALYSIS:
        - Determine if expense is business-related
        - Assess reasonableness of amount
        - Identify any personal use portions
        - Note any limitations on deductibility

        Provide your analysis in JSON format including tax implications.
        """

    @staticmethod
    def build_receipt_prompt(document_content: str, context: PromptContext) -> str:
        """Build prompt for receipt analysis."""

        return f"""
        Analyze this receipt for business expense deduction in {context.jurisdiction.upper()}.

        DOCUMENT CONTENT:
        {document_content}

        EXPENSE ANALYSIS:
        1. Business expense classification
        2. Amount and tax components
        3. Date and merchant details
        4. Business purpose justification
        5. GST/HST input tax credit potential

        CRA REQUIREMENTS:
        - Receipt must show: vendor name, date, amount, GST/HST
        - Business purpose must be evident
        - Payment method should be identifiable
        - Receipt must be legible and complete

        EXPENSE CATEGORIES TO CONSIDER:
        - Vehicle expenses (fuel, maintenance, insurance)
        - Office expenses (supplies, utilities, rent)
        - Meals and entertainment (50% deductible limit)
        - Professional services (legal, accounting)
        - Travel expenses (accommodation, transportation)
        - Advertising and promotion

        Provide analysis with expense recommendations and any concerns.
        """
```

### 3. Tax Q&A Prompts
```python
# ✅ GOOD - Context-aware tax Q&A prompts
# app/llm/prompts/tax_qa.py

class TaxQAPrompts:
    """Prompts for tax-related question answering."""

    @staticmethod
    def build_tax_qa_prompt(
        question: str,
        context_documents: List[str],
        jurisdiction: str,
        user_expertise: str = "general"
    ) -> str:
        """Build prompt for tax Q&A with document context."""

        context_text = "\n\n".join(
            f"DOCUMENT {i+1}:\n{doc[:2000]}..."  # Limit context length
            for i, doc in enumerate(context_documents)
        )

        expertise_level = {
            "beginner": "Use simple language, avoid technical jargon, provide step-by-step explanations",
            "intermediate": "Use standard tax terminology, assume basic tax knowledge",
            "expert": "Use precise tax language, include technical details and references"
        }.get(user_expertise, "Use clear, accessible language")

        return f"""
        You are a Canadian tax expert answering questions for {jurisdiction.upper()} jurisdiction.

        USER EXPERTISE LEVEL: {user_expertise}
        COMMUNICATION STYLE: {expertise_level}

        PROVIDED CONTEXT:
        {context_text if context_text else "No specific documents provided."}

        USER QUESTION:
        {question}

        ANSWERING GUIDELINES:
        1. Base your answer primarily on the provided context when available
        2. Use your general Canadian tax knowledge to supplement the context
        3. Provide specific, actionable advice when possible
        4. Include relevant tax implications and considerations
        5. Mention important deadlines or filing requirements
        6. Suggest when professional tax advice might be needed

        STRUCTURE YOUR ANSWER:
        - Direct answer to the question
        - Explanation with relevant details
        - Tax implications or considerations
        - Action items or next steps
        - Disclaimer about professional advice

        TAX DISCLAIMER:
        This information is for educational purposes only and does not constitute professional tax advice.
        Tax laws are complex and individual circumstances vary.
        Consult with a qualified tax professional for advice specific to your situation.

        Provide a comprehensive, helpful response.
        """

    @staticmethod
    def build_followup_prompt(
        original_question: str,
        original_answer: str,
        followup_question: str,
        jurisdiction: str
    ) -> str:
        """Build prompt for follow-up questions."""

        return f"""
        CONTINUING TAX CONSULTATION for {jurisdiction.upper()} jurisdiction.

        PREVIOUS QUESTION:
        {original_question}

        PREVIOUS ANSWER:
        {original_answer}

        FOLLOW-UP QUESTION:
        {followup_question}

        GUIDELINES:
        1. Reference the previous conversation context
        2. Provide more detailed information based on the follow-up
        3. Maintain consistency with previous advice
        4. Offer additional relevant insights
        5. Suggest related considerations the user might not have thought of

        Continue the conversation naturally while providing valuable tax information.
        """
```

## Cost Optimization Strategies

### 1. Smart Model Selection
```python
# ✅ GOOD - Cost-optimized model selection
# app/llm/cost_optimizer.py

class CostOptimizer:
    """Optimizes LLM usage for cost efficiency."""

    def __init__(self):
        self.model_costs = {
            # Cost per 1K tokens (approximate)
            "gpt-4o": {"input": 0.005, "output": 0.015},
            "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
            "claude-3-5-sonnet": {"input": 0.003, "output": 0.015},
            "claude-3-5-haiku": {"input": 0.00025, "output": 0.00125},
            "glm-4.6": {"input": 0.0001, "output": 0.0002},
            "glm-4-flash": {"input": 0.00005, "output": 0.0001}
        }

        self.daily_budget = 50.0  # USD
        self.usage_tracker = UsageTracker()

    def select_optimal_model(
        self,
        task_type: str,
        complexity: str,  # simple, moderate, complex
        user_tier: str = "standard"  # standard, premium, enterprise
    ) -> str:
        """Select the most cost-effective model for the task."""

        if user_tier == "enterprise":
            # Enterprise users get best models regardless of cost
            return "gpt-4o" if task_type == "vision_analysis" else "gpt-4o-mini"

        if task_type == "vision_analysis" and complexity == "complex":
            # Complex document analysis needs best models
            return "gpt-4o"

        if task_type == "tax_qa" and complexity == "simple":
            # Simple questions can use cheaper models
            return "glm-4-flash"

        if task_type == "entity_extraction":
            # Structured extraction can use mid-range models
            return "claude-3-5-haiku"

        # Default balanced choice
        return "gpt-4o-mini"

    async def check_budget_constraints(self, estimated_cost: float) -> bool:
        """Check if operation fits within budget constraints."""

        current_daily_usage = await self.usage_tracker.get_daily_usage()
        remaining_budget = self.daily_budget - current_daily_usage

        return estimated_cost <= remaining_budget

    def estimate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """Estimate cost for LLM operation."""

        costs = self.model_costs.get(model, self.model_costs["gpt-4o-mini"])
        input_cost = (input_tokens / 1000) * costs["input"]
        output_cost = (output_tokens / 1000) * costs["output"]

        return input_cost + output_cost
```

### 2. Token Optimization
```python
# ✅ GOOD - Token usage optimization
# app/llm/token_optimizer.py

class TokenOptimizer:
    """Optimizes token usage for cost efficiency."""

    @staticmethod
    def compress_document_content(content: str, max_tokens: int = 8000) -> str:
        """Compress document content while preserving tax-relevant information."""

        if len(content) <= max_tokens * 4:  # Rough token estimation
            return content

        # Priority-based content extraction
        sections = {
            "high": [],
            "medium": [],
            "low": []
        }

        lines = content.split('\n')

        for line in lines:
            line_lower = line.lower()

            # High priority: Numbers, amounts, tax terms
            if any(keyword in line_lower for keyword in [
                '$', 'total', 'tax', 'income', 'deduction', 'refund',
                't4', 't5', 't2202', 't2125', 't1', 'schedule'
            ]):
                sections["high"].append(line)

            # Medium priority: Dates, names, addresses
            elif any(keyword in line_lower for keyword in [
                'date', 'address', 'name', 's.i.n.', 'social insurance'
            ]):
                sections["medium"].append(line)

            # Low priority: General text, headers, footers
            elif len(line.strip()) > 10:  # Skip very short lines
                sections["low"].append(line)

        # Reconstruct content prioritizing tax-relevant information
        compressed_content = (
            '\n'.join(sections["high"]) +
            '\n\n' + '\n'.join(sections["medium"][:100]) +  # Limit medium priority
            '\n\n' + '\n'.join(sections["low"][:50])  # Limit low priority
        )

        # If still too long, truncate from the end
        if len(compressed_content) > max_tokens * 4:
            compressed_content = compressed_content[:max_tokens * 4] + "..."

        return compressed_content

    @staticmethod
    def optimize_prompt_template(template: str) -> str:
        """Optimize prompt template for token efficiency."""

        # Remove redundant explanations
        template = re.sub(r'\n{3,}', '\n\n', template)  # Reduce multiple newlines

        # Use concise language
        replacements = {
            "Please make sure to": "Ensure",
            "It is important to": "",
            "You should": "",
            "The user wants you to": "",
            "Your task is to": "",
        }

        for old, new in replacements.items():
            template = template.replace(old, new)

        # Remove verbose examples unless essential
        template = re.sub(r'Example:[\s\S]*?(?=\n\n|\Z)', '', template)

        return template.strip()
```

### 3. Response Caching
```python
# ✅ GOOD - Intelligent response caching
# app/llm/cache.py

import hashlib
import json
from typing import Optional, Dict, Any
from datetime import datetime, timedelta

class LLMResponseCache:
    """Caches LLM responses to avoid redundant calls."""

    def __init__(self):
        self.cache = {}  # In production, use Redis
        self.cache_ttl = {
            "document_analysis": timedelta(hours=24),  # Documents don't change
            "tax_qa": timedelta(hours=1),  # Tax advice can be time-sensitive
            "entity_extraction": timedelta(hours=12)
        }

    def _generate_cache_key(
        self,
        prompt_hash: str,
        model: str,
        parameters: Dict[str, Any]
    ) -> str:
        """Generate cache key for LLM request."""

        key_data = {
            "prompt_hash": prompt_hash,
            "model": model,
            "parameters": parameters
        }

        key_string = json.dumps(key_data, sort_keys=True)
        return hashlib.sha256(key_string.encode()).hexdigest()

    def _hash_prompt(self, prompt: str) -> str:
        """Hash prompt for cache key generation."""

        # Normalize prompt (remove extra whitespace, etc.)
        normalized = re.sub(r'\s+', ' ', prompt.strip())
        return hashlib.sha256(normalized.encode()).hexdigest()

    async def get(
        self,
        prompt: str,
        model: str,
        task_type: str,
        parameters: Dict[str, Any] = None
    ) -> Optional[Dict[str, Any]]:
        """Get cached response if available."""

        cache_key = self._generate_cache_key(
            self._hash_prompt(prompt),
            model,
            parameters or {}
        )

        cached_item = self.cache.get(cache_key)

        if not cached_item:
            return None

        # Check if cache entry is still valid
        ttl = self.cache_ttl.get(task_type, timedelta(hours=1))
        if datetime.utcnow() - cached_item["timestamp"] > ttl:
            del self.cache[cache_key]
            return None

        return cached_item["response"]

    async def set(
        self,
        prompt: str,
        model: str,
        task_type: str,
        response: Dict[str, Any],
        parameters: Dict[str, Any] = None
    ):
        """Cache LLM response."""

        cache_key = self._generate_cache_key(
            self._hash_prompt(prompt),
            model,
            parameters or {}
        )

        self.cache[cache_key] = {
            "response": response,
            "timestamp": datetime.utcnow(),
            "task_type": task_type
        }

    def clear_expired(self):
        """Clear expired cache entries."""

        current_time = datetime.utcnow()
        expired_keys = []

        for key, value in self.cache.items():
            task_type = value["task_type"]
            ttl = self.cache_ttl.get(task_type, timedelta(hours=1))

            if current_time - value["timestamp"] > ttl:
                expired_keys.append(key)

        for key in expired_keys:
            del self.cache[key]
```

## Quality Assurance

### 1. Response Validation
```python
# ✅ GOOD - LLM response validation
# app/llm/validation.py

class ResponseValidator:
    """Validates LLM responses for quality and correctness."""

    @staticmethod
    def validate_document_analysis(response: Dict[str, Any]) -> ValidationResult:
        """Validate document analysis response."""

        errors = []
        warnings = []

        # Check required fields
        required_fields = ["document_type", "tax_year", "extracted_data"]
        for field in required_fields:
            if field not in response:
                errors.append(f"Missing required field: {field}")

        # Validate tax year
        if "tax_year" in response:
            tax_year = response["tax_year"]
            if not isinstance(tax_year, int) or tax_year < 2015 or tax_year > 2025:
                errors.append(f"Invalid tax year: {tax_year}")

        # Validate confidence scores
        if "confidence_scores" in response:
            confidence = response["confidence_scores"]
            if "overall" in confidence:
                overall_conf = confidence["overall"]
                if not isinstance(overall_conf, (int, float)) or not (0 <= overall_conf <= 1):
                    errors.append(f"Invalid overall confidence score: {overall_conf}")

                if overall_conf < 0.5:
                    warnings.append("Low confidence score - results may be unreliable")

        # Validate extracted data types
        if "extracted_data" in response:
            extracted = response["extracted_data"]
            monetary_fields = ["total_income", "tax_paid", "business_expenses"]

            for field in monetary_fields:
                if field in extracted and extracted[field] is not None:
                    value = extracted[field]
                    if not isinstance(value, (int, float)) or value < 0:
                        errors.append(f"Invalid monetary value for {field}: {value}")

        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            warnings=warnings
        )

    @staticmethod
    def validate_tax_qa_response(response: str, question: str) -> ValidationResult:
        """Validate tax Q&A response."""

        errors = []
        warnings = []

        # Check minimum length
        if len(response) < 50:
            errors.append("Response too short - may not fully answer the question")

        # Check for disclaimer
        if "professional tax advice" not in response.lower():
            warnings.append("Missing professional advice disclaimer")

        # Check for actionable content
        if not any(indicator in response.lower() for indicator in [
            "should", "recommend", "consider", "ensure", "file", "deadline"
        ]):
            warnings.append("Response may lack actionable advice")

        # Check for specific amounts or references
        if has_money_question(question) and not has_amount_in_response(response):
            warnings.append("Question about amounts but no specific figures in response")

        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            warnings=warnings
        )
```

## Monitoring and Analytics

### 1. Usage Tracking
```python
# ✅ GOOD - Comprehensive usage tracking
# app/llm/monitoring.py

class LLMUsageTracker:
    """Tracks LLM usage for monitoring and optimization."""

    def __init__(self):
        self.usage_data = []

    async def track_request(
        self,
        provider: str,
        model: str,
        task_type: str,
        input_tokens: int,
        output_tokens: int,
        response_time: float,
        success: bool,
        error_type: str = None,
        user_id: str = None
    ):
        """Track LLM request for analytics."""

        usage_record = {
            "timestamp": datetime.utcnow().isoformat(),
            "provider": provider,
            "model": model,
            "task_type": task_type,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": input_tokens + output_tokens,
            "response_time_ms": response_time * 1000,
            "success": success,
            "error_type": error_type,
            "user_id": user_id
        }

        self.usage_data.append(usage_record)

        # Also send to monitoring service
        await self.send_to_monitoring(usage_record)

    def get_daily_usage(self) -> Dict[str, Any]:
        """Get usage statistics for current day."""

        today = datetime.utcnow().date()
        today_records = [
            record for record in self.usage_data
            if datetime.fromisoformat(record["timestamp"]).date() == today
        ]

        total_tokens = sum(record["total_tokens"] for record in today_records)
        total_cost = self._calculate_daily_cost(today_records)

        return {
            "date": today.isoformat(),
            "total_requests": len(today_records),
            "successful_requests": sum(1 for r in today_records if r["success"]),
            "failed_requests": sum(1 for r in today_records if not r["success"]),
            "total_tokens": total_tokens,
            "estimated_cost_usd": total_cost,
            "average_response_time_ms": sum(r["response_time_ms"] for r in today_records) / len(today_records) if today_records else 0
        }

    def _calculate_daily_cost(self, records: List[Dict]) -> float:
        """Calculate estimated cost for usage records."""

        cost_optimizer = CostOptimizer()
        total_cost = 0.0

        for record in records:
            cost = cost_optimizer.estimate_cost(
                record["model"],
                record["input_tokens"],
                record["output_tokens"]
            )
            total_cost += cost

        return total_cost
```

## Related Skills
- **[backend-development](backend-development)** - Backend LLM integration
- **[configuration-management](configuration-management)** - LLM provider configuration
- **[error-handling-transparency](error-handling-transparency)** - LLM error handling
- **[testing-strategy](testing-strategy)** - LLM integration testing
- **[core-architecture](core-architecture)** - LLM system architecture

## Usage Hints
Trigger this skill when:
- Integrating new LLM providers
- Designing prompts for document analysis
- Implementing tax Q&A functionality
- Optimizing LLM usage costs
- Setting up LLM monitoring
- Debugging LLM responses
- Planning LLM fallback strategies
- Implementing response caching

## Quality Checklist
- [ ] Multiple LLM providers configured with fallback
- [ ] Prompts are modular and context-aware
- [ ] Response validation is implemented
- [ ] Cost optimization strategies are in place
- [ ] Usage monitoring and tracking is set up
- [ ] Error handling includes LLM-specific cases
- [ ] Cache invalidation policies are defined
- [ ] Rate limiting is implemented per provider
- [ ] Response quality metrics are tracked
- [ ] Budget constraints and alerts are configured