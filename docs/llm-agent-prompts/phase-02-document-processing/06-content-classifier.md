# ä»»åŠ¡06ï¼šå†…å®¹åˆ†ç±»æ¨¡å—å¼€å‘

## ä»»åŠ¡ç›®æ ‡

å¼€å‘ä¸€ä¸ªæ™ºèƒ½å†…å®¹åˆ†ç±»æ¨¡å—ï¼Œå°†ä» PDF æå–çš„ç»“æ„åŒ–å†…å®¹æŒ‰ç…§ CRA ç¨åŠ¡æ–‡æ¡£çš„é€»è¾‘è¿›è¡Œåˆ†ç±»ï¼Œä¸ºåç»­çš„ Skill ç”Ÿæˆåšå‡†å¤‡ã€‚è¯¥æ¨¡å—éœ€è¦ç†è§£ç¨åŠ¡æ–‡æ¡£çš„ç»“æ„ï¼Œè‡ªåŠ¨è¯†åˆ«ä¸åŒç±»å‹çš„å†…å®¹ã€‚

**æŠ€æœ¯å‡çº§**: é›†æˆ Skill_Seekers çš„å…ˆè¿›åˆ†ç±»æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¤šæºç»Ÿä¸€å¤„ç†ã€æ™ºèƒ½åˆ†ç±»è¯„åˆ†å’Œå†²çªæ£€æµ‹æœºåˆ¶ï¼Œæ˜¾è‘—æå‡åˆ†ç±»å‡†ç¡®æ€§å’Œå†…å®¹è´¨é‡ã€‚

## æŠ€æœ¯è¦æ±‚

**æ ¸å¿ƒåº“ï¼š**
- `scikit-learn`ï¼šæ–‡æœ¬åˆ†ç±»ç®—æ³•ï¼ˆåŸºäº Skill_Seekers çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼‰
- `nltk/spacy`ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆå¢å¼ºçš„å®ä½“è¯†åˆ«ï¼‰
- `re`ï¼šæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
- `json`ï¼šæ•°æ®ç»“æ„å¤„ç†
- `numpy`ï¼šæ•°å€¼è®¡ç®—å’Œè¯„åˆ†ç®—æ³•
- `sklearn.feature_extraction`ï¼šç‰¹å¾æå–å’Œå‘é‡åŒ–ï¼ˆæ¥è‡ª Skill_Seekersï¼‰

**åˆ†ç±»ç›®æ ‡ï¼š**
- æŒ‰ç¨åŠ¡ä¸»é¢˜åˆ†ç±»ï¼ˆæ”¶å…¥ç±»å‹ã€æŠµæ‰£ã€ç¨æ”¶ä¼˜æƒ ç­‰ï¼‰
- æŒ‰å†…å®¹ç±»å‹åˆ†ç±»ï¼ˆæ³•è§„æ¡æ¬¾ã€è®¡ç®—ç¤ºä¾‹ã€è¡¨æ ¼æ•°æ®ï¼‰
- æŒ‰é‡è¦ç¨‹åº¦åˆ†ç±»ï¼ˆæ ¸å¿ƒæ³•è§„ã€è¾…åŠ©è¯´æ˜ã€å‚è€ƒä¿¡æ¯ï¼‰
- æŒ‰ç”¨æˆ·éœ€æ±‚åˆ†ç±»ï¼ˆä¸ªäººæŠ¥ç¨ã€ä¼ä¸šç¨åŠ¡ã€æŠ•èµ„ç¨åŠ¡ï¼‰

**Skill_Seekers é›†æˆç‰¹æ€§ï¼š**
- **å¤šæºç»Ÿä¸€å¤„ç†**ï¼šè¯†åˆ«å’Œå¤„ç†æ¥è‡ªä¸åŒ CRA æ–‡æ¡£æºçš„å†…å®¹
- **æ™ºèƒ½è¯„åˆ†ç®—æ³•**ï¼šåŸºäº TF-IDF å’Œè¯­ä¹‰ç›¸ä¼¼åº¦çš„åˆ†ç±»è¯„åˆ†
- **å†²çªæ£€æµ‹æœºåˆ¶**ï¼šè¯†åˆ«æ–‡æ¡£é—´çš„çŸ›ç›¾å’Œè¿‡æ—¶ä¿¡æ¯
- **è‡ªé€‚åº”åˆ†ç±»**ï¼šåŸºäºå†…å®¹ç‰¹å¾çš„åŠ¨æ€åˆ†ç±»é˜ˆå€¼è°ƒæ•´
- **äº¤å‰éªŒè¯**ï¼šå¤šç»´åº¦éªŒè¯åˆ†ç±»ç»“æœçš„å‡†ç¡®æ€§

**è¾“å‡ºè¦æ±‚ï¼š**
- ç»“æ„åŒ–çš„å†…å®¹åˆ†ç±»
- åˆ†ç±»ç½®ä¿¡åº¦è¯„åˆ†ï¼ˆå¢å¼ºç®—æ³•ï¼‰
- äº¤å‰å¼•ç”¨å…³ç³»ï¼ˆæ™ºèƒ½å…³è”ï¼‰
- ä¼˜å…ˆçº§æ’åºï¼ˆå¤šå› å­è¯„åˆ†ï¼‰
- å†…å®¹è´¨é‡è¯„åˆ†ï¼ˆæ–°å¢ï¼‰
- å†²çªæ£€æµ‹ç»“æœï¼ˆæ–°å¢ï¼‰

## å®ç°æ­¥éª¤

### 1. åˆ›å»ºå¢å¼ºåˆ†ç±»å™¨æ¶æ„

**åŸºäº Skill_Seekers çš„åˆ†ç±»å™¨è®¾è®¡ï¼š**

```python
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional, Set
from enum import Enum
import re
import json
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime

class TaxCategory(Enum):
    """ç¨åŠ¡åˆ†ç±»æšä¸¾"""
    BUSINESS_INCOME = "business_income"
    CAPITAL_GAINS = "capital_gains"
    RENTAL_INCOME = "rental_income"
    EMPLOYMENT_INCOME = "employment_income"
    INVESTMENT_INCOME = "investment_income"
    DEDUCTIONS = "deductions"
    TAX_CREDITS = "tax_credits"
    RRSP = "rrsp"
    GST_HST = "gst_hst"
    RECORD_KEEPING = "record_keeping"
    FILING_REQUIREMENTS = "filing_requirements"

class ContentType(Enum):
    """å†…å®¹ç±»å‹æšä¸¾"""
    REGULATION = "regulation"      # æ³•è§„æ¡æ¬¾
    CALCULATION = "calculation"    # è®¡ç®—æ–¹æ³•
    EXAMPLE = "example"           # ç¤ºä¾‹è¯´æ˜
    TABLE = "table"              # è¡¨æ ¼æ•°æ®
    FORM = "form"                # è¡¨æ ¼è¯´æ˜
    DEFINITION = "definition"     # å®šä¹‰è¯´æ˜
    FAQ = "faq"                  # å¸¸è§é—®é¢˜
    REFERENCE = "reference"       # å‚è€ƒä¿¡æ¯

class Priority(Enum):
    """ä¼˜å…ˆçº§æšä¸¾"""
    CRITICAL = 1    # æ ¸å¿ƒæ³•è§„
    HIGH = 2        # é‡è¦è¯´æ˜
    MEDIUM = 3      # è¾…åŠ©ä¿¡æ¯
    LOW = 4         # å‚è€ƒå†…å®¹

@dataclass
class ContentQualityMetrics:
    """å†…å®¹è´¨é‡æŒ‡æ ‡ï¼ˆæ¥è‡ª Skill_Seekersï¼‰"""
    completeness_score: float = 0.0  # å®Œæ•´æ€§è¯„åˆ†
    accuracy_score: float = 0.0     # å‡†ç¡®æ€§è¯„åˆ†
    relevance_score: float = 0.0    # ç›¸å…³æ€§è¯„åˆ†
    freshness_score: float = 0.0    # æ—¶æ•ˆæ€§è¯„åˆ†
    clarity_score: float = 0.0      # æ¸…æ™°åº¦è¯„åˆ†
    overall_quality: float = 0.0    # ç»¼åˆè´¨é‡è¯„åˆ†

@dataclass
class ConflictDetection:
    """å†²çªæ£€æµ‹ç»“æœï¼ˆæ¥è‡ª Skill_Seekersï¼‰"""
    has_conflicts: bool = False
    conflict_type: str = ""         # å†²çªç±»å‹
    conflict_description: str = ""  # å†²çªæè¿°
    severity: str = ""             # ä¸¥é‡ç¨‹åº¦ï¼šlow/medium/high/critical
    suggested_resolution: str = ""  # å»ºè®®è§£å†³æ–¹æ¡ˆ
    conflicting_sources: List[str] = field(default_factory=list)

@dataclass
class ClassifiedContent:
    """å¢å¼ºçš„åˆ†ç±»åå†…å®¹"""
    content_id: str
    original_content: Dict
    tax_category: TaxCategory
    content_type: ContentType
    priority: Priority
    confidence_score: float
    keywords: List[str]
    cross_references: List[str]
    summary: str
    target_audience: List[str]

    # Skill_Seekers å¢å¼ºå­—æ®µ
    quality_metrics: ContentQualityMetrics = field(default_factory=ContentQualityMetrics)
    conflict_detection: ConflictDetection = field(default_factory=ConflictDetection)
    source_reliability: float = 0.0    # æ¥æºå¯é æ€§è¯„åˆ†
    semantic_similarity: Dict[str, float] = field(default_factory=dict)
    classification_path: List[str] = field(default_factory=list)  # åˆ†ç±»è·¯å¾„
    verification_status: str = "pending"  # éªŒè¯çŠ¶æ€
    last_updated: datetime = field(default_factory=datetime.now)
```

### 2. å®ç°å¢å¼ºæ ¸å¿ƒåˆ†ç±»å™¨

**é›†æˆ Skill_Seekers çš„æ™ºèƒ½åˆ†ç±»ç®—æ³•ï¼š**

```python
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics.pairwise import cosine_similarity
import spacy

class EnhancedTaxContentClassifier:
    """å¢å¼ºçš„ç¨åŠ¡å†…å®¹åˆ†ç±»å™¨ï¼ˆé›†æˆ Skill_Seekers æŠ€æœ¯ï¼‰"""

    def __init__(self):
        # åŸºç¡€åˆ†ç±»å™¨é…ç½®
        self.confidence_threshold = 0.6  # åŠ¨æ€è°ƒæ•´çš„ç½®ä¿¡åº¦é˜ˆå€¼
        self.conflict_detection_enabled = True
        self.quality_assessment_enabled = True

        # Skill_Seekers ç‰¹æ€§
        self.tfidf_vectorizer = None
        self.content_corpus = []  # å†…å®¹è¯­æ–™åº“
        self.classification_history = []  # åˆ†ç±»å†å²è®°å½•

        # åŠ è½½ NLP æ¨¡å‹
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except OSError:
            print("âš ï¸ spaCy æ¨¡å‹æœªå®‰è£…ï¼Œä½¿ç”¨åŸºç¡€åˆ†ç±»")
            self.nlp = None

        # åˆå§‹åŒ– Skill_Seekers ç»„ä»¶
        self._init_skill_seekers_components()

    def _init_skill_seekers_components(self):
        """åˆå§‹åŒ– Skill_Seekers ç»„ä»¶"""
        # åˆå§‹åŒ– TF-IDF å‘é‡åŒ–å™¨
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.8
        )

        # CRA ç¨åŠ¡å…³é”®è¯åº“ï¼ˆå¢å¼ºç‰ˆï¼‰
        self.tax_keywords = {
            TaxCategory.BUSINESS_INCOME: [
                "business income", "self-employment", "sole proprietorship",
                "partnership", "business expenses", "professional fees",
                "business losses", "home office expenses"
            ],
            TaxCategory.CAPITAL_GAINS: [
                "capital gains", "capital property", "disposition", "principal residence",
                "taxable capital gains", "capital losses", "adjusted cost base",
                "inclusion rate", "capital gains exemption"
            ],
            # ... å…¶ä»–åˆ†ç±»ä¿æŒä¸å˜
        }

        # Skill_Seekers æ™ºèƒ½è¯„åˆ†æƒé‡
        self.scoring_weights = {
            'keyword_match': 0.4,      # å…³é”®è¯åŒ¹é…æƒé‡
            'semantic_similarity': 0.3,  # è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡
            'context_relevance': 0.2,   # ä¸Šä¸‹æ–‡ç›¸å…³æ€§æƒé‡
            'historical_accuracy': 0.1   # å†å²å‡†ç¡®æ€§æƒé‡
        }

    def classify_content_with_intelligence(self, extracted_data: Dict) -> List[ClassifiedContent]:
        """æ™ºèƒ½å†…å®¹åˆ†ç±»ï¼ˆSkill_Seekers å¢å¼ºç‰ˆï¼‰"""
        classified_contents = []

        # æ„å»ºå†…å®¹è¯­æ–™åº“ç”¨äºè¯­ä¹‰åˆ†æ
        self._build_content_corpus(extracted_data)

        for page_data in extracted_data.get('pages', []):
            # åˆ†ç±»å•é¡µå†…å®¹
            classified = self._classify_page_enhanced(page_data)
            classified_contents.extend(classified)

        # å¤„ç†è¡¨æ ¼æ•°æ®
        for table in extracted_data.get('tables', []):
            classified = self._classify_table_enhanced(table)
            classified_contents.append(classified)

        # Skill_Seekers å¢å¼ºï¼šå†²çªæ£€æµ‹
        if self.conflict_detection_enabled:
            self._detect_conflicts(classified_contents)

        # Skill_Seekers å¢å¼ºï¼šè´¨é‡è¯„ä¼°
        if self.quality_assessment_enabled:
            self._assess_content_quality(classified_contents)

        # å»ºç«‹æ™ºèƒ½äº¤å‰å¼•ç”¨
        self._establish_intelligent_cross_references(classified_contents)

        # åŠ¨æ€è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼
        self._adjust_confidence_threshold(classified_contents)

        # æŒ‰ä¼˜å…ˆçº§å’Œç»¼åˆè¯„åˆ†æ’åº
        classified_contents.sort(key=lambda x: self._calculate_overall_score(x), reverse=True)

        return classified_contents

    def _classify_page_enhanced(self, page_data: Dict) -> List[ClassifiedContent]:
        """å¢å¼ºçš„å•é¡µåˆ†ç±»ï¼ˆé›†æˆ Skill_Seekers ç®—æ³•ï¼‰"""
        contents = []
        paragraphs = self._split_into_paragraphs(page_data['text'])

        for i, paragraph in enumerate(paragraphs):
            if len(paragraph.strip()) < 20:
                continue

            # åŸºç¡€åˆ†ç±»
            tax_category, tax_confidence = self._classify_tax_category_enhanced(paragraph)
            content_type, type_confidence = self._classify_content_type_enhanced(paragraph)
            priority, priority_confidence = self._classify_priority_enhanced(paragraph, tax_category)

            # Skill_Seekers å¢å¼ºï¼šè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ
            semantic_scores = self._calculate_semantic_similarity(paragraph)

            # Skill_Seekers å¢å¼ºï¼šå¤šå› å­ç»¼åˆè¯„åˆ†
            overall_confidence = self._calculate_enhanced_confidence(
                tax_confidence, type_confidence, priority_confidence, semantic_scores
            )

            # åˆ›å»ºå¢å¼ºçš„åˆ†ç±»å†…å®¹å¯¹è±¡
            content = ClassifiedContent(
                content_id=f"page_{page_data['page_number']}_para_{i}",
                original_content=page_data,
                tax_category=tax_category,
                content_type=content_type,
                priority=priority,
                confidence_score=overall_confidence,
                keywords=self._extract_keywords_enhanced(paragraph),
                cross_references=[],
                summary=self._generate_summary_enhanced(paragraph),
                target_audience=self._identify_target_audience_enhanced(paragraph),
                semantic_similarity=semantic_scores,
                classification_path=self._generate_classification_path(tax_category, content_type)
            )

            contents.append(content)

        return contents

    def _classify_tax_category_enhanced(self, text: str) -> Tuple[TaxCategory, float]:
        """å¢å¼ºçš„ç¨åŠ¡ä¸»é¢˜åˆ†ç±»"""
        text_lower = text.lower()
        category_scores = {}

        # å…³é”®è¯åŒ¹é…è¯„åˆ†
        for category, keywords in self.tax_keywords.items():
            keyword_score = sum(text_lower.count(kw) for kw in keywords)
            category_scores[category] = keyword_score * self.scoring_weights['keyword_match']

        # è¯­ä¹‰ç›¸ä¼¼åº¦è¯„åˆ†
        if self.tfidf_vectorizer and self.content_corpus:
            semantic_scores = self._calculate_semantic_similarity(text)
            for category, score in semantic_scores.items():
                if category in category_scores:
                    category_scores[category] += score * self.scoring_weights['semantic_similarity']

        # ä¸Šä¸‹æ–‡ç›¸å…³æ€§è¯„åˆ†
        context_scores = self._calculate_context_relevance(text)
        for category, score in context_scores.items():
            if category in category_scores:
                category_scores[category] += score * self.scoring_weights['context_relevance']

        # å†å²å‡†ç¡®æ€§è¯„åˆ†
        history_scores = self._calculate_historical_accuracy(text)
        for category, score in history_scores.items():
            if category in category_scores:
                category_scores[category] += score * self.scoring_weights['historical_accuracy']

        # é€‰æ‹©æœ€é«˜åˆ†çš„åˆ†ç±»
        if not category_scores or max(category_scores.values()) == 0:
            return TaxCategory.BUSINESS_INCOME, 0.1

        best_category = max(category_scores, key=category_scores.get)
        max_score = category_scores[best_category]
        confidence = min(max_score / 3.0, 1.0)  # å½’ä¸€åŒ–ç½®ä¿¡åº¦

        return best_category, confidence

    def _calculate_semantic_similarity(self, text: str) -> Dict[str, float]:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆSkill_Seekers æ ¸å¿ƒç®—æ³•ï¼‰"""
        if not self.tfidf_vectorizer or not self.content_corpus:
            return {}

        try:
            # å‘é‡åŒ–å½“å‰æ–‡æœ¬
            text_vector = self.tfidf_vectorizer.transform([text])

            # è®¡ç®—ä¸è¯­æ–™åº“çš„ç›¸ä¼¼åº¦
            similarities = {}
            for category, corpus_text in self.content_corpus.items():
                corpus_vector = self.tfidf_vectorizer.transform([corpus_text])
                similarity = cosine_similarity(text_vector, corpus_vector)[0][0]
                similarities[category] = float(similarity)

            return similarities
        except Exception as e:
            print(f"âš ï¸ è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return {}

    def _detect_conflicts(self, contents: List[ClassifiedContent]):
        """æ£€æµ‹å†…å®¹å†²çªï¼ˆæ¥è‡ª Skill_Seekers å†²çªæ£€æµ‹æœºåˆ¶ï¼‰"""
        for i, content1 in enumerate(contents):
            for j, content2 in enumerate(contents[i+1:], i+1):
                conflict = self._analyze_content_conflict(content1, content2)
                if conflict.has_conflicts:
                    # æ ‡è®°å†²çªå†…å®¹
                    if not content1.conflict_detection.has_conflicts:
                        content1.conflict_detection = conflict
                    if not content2.conflict_detection.has_conflicts:
                        content2.conflict_detection = conflict

    def _analyze_content_conflict(self, content1: ClassifiedContent, content2: ClassifiedContent) -> ConflictDetection:
        """åˆ†æä¸¤ä¸ªå†…å®¹ä¹‹é—´çš„å†²çª"""
        conflict = ConflictDetection()

        # æ£€æŸ¥ç›¸åŒä¸»é¢˜çš„ä¸åŒè¡¨è¿°
        if content1.tax_category == content2.tax_category:
            # æ£€æŸ¥æ•°å€¼å†²çª
            numbers1 = self._extract_numbers(content1.summary)
            numbers2 = self._extract_numbers(content2.summary)

            if numbers1 and numbers2:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾è‘—çš„æ•°å€¼å·®å¼‚
                for num1 in numbers1:
                    for num2 in numbers2:
                        if abs(num1 - num2) > max(num1, num2) * 0.1:  # 10% å·®å¼‚é˜ˆå€¼
                            conflict.has_conflicts = True
                            conflict.conflict_type = "numerical_discrepancy"
                            conflict.severity = "medium"
                            conflict.conflict_description = f"æ•°å€¼å†²çª: {num1} vs {num2}"
                            conflict.conflicting_sources = [content1.content_id, content2.content_id]

        # æ£€æŸ¥æ—¶é—´ç›¸å…³æ€§å†²çª
        if self._has_temporal_conflict(content1, content2):
            conflict.has_conflicts = True
            conflict.conflict_type = "temporal_conflict"
            conflict.severity = "low"
            conflict.conflict_description = "æ—¶é—´ç›¸å…³æ€§å¯èƒ½å­˜åœ¨å†²çª"

        return conflict

    def _assess_content_quality(self, contents: List[ClassifiedContent]):
        """è¯„ä¼°å†…å®¹è´¨é‡ï¼ˆæ¥è‡ª Skill_Seekers è´¨é‡è¯„ä¼°ç³»ç»Ÿï¼‰"""
        for content in contents:
            quality = ContentQualityMetrics()

            # å®Œæ•´æ€§è¯„åˆ†
            quality.completeness_score = self._assess_completeness(content)

            # å‡†ç¡®æ€§è¯„åˆ†
            quality.accuracy_score = self._assess_accuracy(content)

            # ç›¸å…³æ€§è¯„åˆ†
            quality.relevance_score = self._assess_relevance(content)

            # æ—¶æ•ˆæ€§è¯„åˆ†
            quality.freshness_score = self._assess_freshness(content)

            # æ¸…æ™°åº¦è¯„åˆ†
            quality.clarity_score = self._assess_clarity(content)

            # ç»¼åˆè´¨é‡è¯„åˆ†
            quality.overall_quality = (
                quality.completeness_score * 0.3 +
                quality.accuracy_score * 0.3 +
                quality.relevance_score * 0.2 +
                quality.freshness_score * 0.1 +
                quality.clarity_score * 0.1
            )

            content.quality_metrics = quality

    def _calculate_enhanced_confidence(self, tax_conf: float, type_conf: float,
                                     priority_conf: float, semantic_scores: Dict) -> float:
        """è®¡ç®—å¢å¼ºç½®ä¿¡åº¦è¯„åˆ†"""
        base_confidence = (tax_conf + type_conf + priority_conf) / 3

        # è¯­ä¹‰ç›¸ä¼¼åº¦å¢å¼º
        if semantic_scores:
            max_semantic_score = max(semantic_scores.values())
            base_confidence = base_confidence * 0.7 + max_semantic_score * 0.3

        return min(base_confidence, 1.0)

    def _calculate_overall_score(self, content: ClassifiedContent) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆç”¨äºæ’åºï¼‰"""
        return (
            content.confidence_score * 0.4 +
            content.quality_metrics.overall_quality * 0.3 +
            (1.0 / content.priority.value) * 0.2 +  # ä¼˜å…ˆçº§è¶Šé«˜åˆ†æ•°è¶Šé«˜
            content.source_reliability * 0.1
        )

    def _build_content_corpus(self, extracted_data: Dict):
        """æ„å»ºå†…å®¹è¯­æ–™åº“ç”¨äºè¯­ä¹‰åˆ†æ"""
        self.content_corpus = {}

        # ä¸ºæ¯ä¸ªç¨åŠ¡åˆ†ç±»æ„å»ºä»£è¡¨æ€§æ–‡æœ¬
        for category in TaxCategory:
            category_keywords = self.tax_keywords.get(category, [])
            if category_keywords:
                self.content_corpus[category] = " ".join(category_keywords)

        # ä»å®é™…å†…å®¹ä¸­å­¦ä¹ 
        all_text = []
        for page_data in extracted_data.get('pages', []):
            all_text.append(page_data.get('text', ''))

        if all_text and len(all_text) > 10:
            # è®­ç»ƒ TF-IDF å‘é‡åŒ–å™¨
            try:
                self.tfidf_vectorizer.fit(all_text)
            except Exception as e:
                print(f"âš ï¸ TF-IDF è®­ç»ƒå¤±è´¥: {e}")

    # ... å…¶ä»–è¾…åŠ©æ–¹æ³•ä¿æŒä¸å˜
    def _classify_content_type_enhanced(self, text: str) -> Tuple[ContentType, float]:
        """å¢å¼ºçš„å†…å®¹ç±»å‹åˆ†ç±»"""
        # ä½¿ç”¨åŸæœ‰çš„åˆ†ç±»é€»è¾‘ï¼Œä½†å¢åŠ è¯„åˆ†æƒé‡
        text_lower = text.lower()
        for content_type, patterns in self.content_patterns.items():
            matches = sum(1 for pattern in patterns if re.search(pattern, text_lower, re.IGNORECASE))
            if matches > 0:
                return content_type, min(matches * 0.2, 0.9)
        return ContentType.REGULATION, 0.3

    def _classify_priority_enhanced(self, text: str, tax_category: TaxCategory) -> Tuple[Priority, float]:
        """å¢å¼ºçš„ä¼˜å…ˆçº§åˆ†ç±»"""
        # ä½¿ç”¨åŸæœ‰çš„ä¼˜å…ˆçº§é€»è¾‘ï¼Œä½†è€ƒè™‘åˆ†ç±»ç½®ä¿¡åº¦
        text_lower = text.lower()

        high_priority_keywords = ["must", "required", "mandatory", "penalty", "deadline"]
        medium_priority_keywords = ["should", "recommended", "guideline", "example"]

        if any(kw in text_lower for kw in high_priority_keywords):
            return Priority.CRITICAL, 0.9
        elif any(kw in text_lower for kw in medium_priority_keywords):
            return Priority.HIGH, 0.7
        else:
            return Priority.MEDIUM, 0.5

    def _extract_keywords_enhanced(self, text: str) -> List[str]:
        """å¢å¼ºçš„å…³é”®è¯æå–"""
        keywords = set()

        # åŸºç¡€å…³é”®è¯æå–
        for category_keywords in self.tax_keywords.values():
            for kw in category_keywords:
                if kw in text.lower():
                    keywords.add(kw)

        # ä½¿ç”¨ spaCy æå–å®ä½“
        if self.nlp:
            doc = self.nlp(text)
            for ent in doc.ents:
                if ent.label_ in ['ORG', 'MONEY', 'DATE', 'GPE']:
                    keywords.add(ent.text.lower())

        return list(keywords)

    def _generate_summary_enhanced(self, text: str) -> str:
        """å¢å¼ºçš„æ‘˜è¦ç”Ÿæˆ"""
        # ç®€åŒ–çš„æ‘˜è¦ç”Ÿæˆé€»è¾‘
        sentences = re.split(r'[.!?]+', text)
        if sentences:
            return sentences[0][:200] + ("..." if len(sentences[0]) > 200 else ".")
        return text[:200] + "..."

    def _identify_target_audience_enhanced(self, text: str) -> List[str]:
        """å¢å¼ºçš„ç›®æ ‡å—ä¼—è¯†åˆ«"""
        text_lower = text.lower()
        audience = []

        audience_patterns = {
            "individual": ["you", "your", "personal", "individual"],
            "business": ["business", "corporation", "company"],
            "accountant": ["accountant", "professional", "advisor"],
            "investor": ["investor", "investment", "portfolio"]
        }

        for audience_type, keywords in audience_patterns.items():
            if any(kw in text_lower for kw in keywords):
                audience.append(audience_type)

        return audience if audience else ["general"]

    def _generate_classification_path(self, category: TaxCategory, content_type: ContentType) -> List[str]:
        """ç”Ÿæˆåˆ†ç±»è·¯å¾„"""
        return [category.value, content_type.value]

    def _establish_intelligent_cross_references(self, contents: List[ClassifiedContent]):
        """å»ºç«‹æ™ºèƒ½äº¤å‰å¼•ç”¨"""
        for content in contents:
            # åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…å»ºç«‹å¼•ç”¨
            for other in contents:
                if content.content_id == other.content_id:
                    continue

                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = 0
                common_keywords = set(content.keywords) & set(other.keywords)
                similarity += len(common_keywords) * 0.1

                # è¯­ä¹‰ç›¸ä¼¼åº¦
                for category, score in content.semantic_similarity.items():
                    if category in other.semantic_similarity:
                        similarity += abs(score - other.semantic_similarity[category]) * 0.2

                if similarity > 0.3:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    content.cross_references.append(other.content_id)

            # é™åˆ¶å¼•ç”¨æ•°é‡
            content.cross_references = content.cross_references[:5]

    def _adjust_confidence_threshold(self, contents: List[ClassifiedContent]):
        """åŠ¨æ€è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼"""
        if not contents:
            return

        avg_confidence = sum(c.confidence_score for c in contents) / len(contents)

        # æ ¹æ®å¹³å‡ç½®ä¿¡åº¦è°ƒæ•´é˜ˆå€¼
        if avg_confidence > 0.8:
            self.confidence_threshold = 0.7
        elif avg_confidence < 0.5:
            self.confidence_threshold = 0.4
        else:
            self.confidence_threshold = 0.6

    def _extract_numbers(self, text: str) -> List[float]:
        """æå–æ–‡æœ¬ä¸­çš„æ•°å­—"""
        import re
        numbers = re.findall(r'\d+\.?\d*', text)
        return [float(num) for num in numbers]

    def _has_temporal_conflict(self, content1: ClassifiedContent, content2: ClassifiedContent) -> bool:
        """æ£€æŸ¥æ—¶é—´å†²çª"""
        # ç®€åŒ–çš„æ—¶é—´å†²çªæ£€æµ‹
        return False  # å®é™…å®ç°ä¼šæ›´å¤æ‚

# Skill_Seekers å¢å¼ºä½¿ç”¨ç¤ºä¾‹
def classify_cra_document_enhanced(extracted_data: Dict, output_dir: str = "output") -> List[ClassifiedContent]:
    """ä½¿ç”¨å¢å¼ºåˆ†ç±»å™¨å¯¹ CRA æ–‡æ¡£è¿›è¡Œåˆ†ç±»"""

    # åˆ›å»ºå¢å¼ºåˆ†ç±»å™¨
    classifier = EnhancedTaxContentClassifier()

    # å¯ç”¨æ‰€æœ‰ Skill_Seekers ç‰¹æ€§
    classifier.conflict_detection_enabled = True
    classifier.quality_assessment_enabled = True

    # æ‰§è¡Œæ™ºèƒ½åˆ†ç±»
    classified_contents = classifier.classify_content_with_intelligence(extracted_data)

    # ç”Ÿæˆå¢å¼ºçš„åˆ†ç±»æŠ¥å‘Š
    classification_report = generate_enhanced_classification_report(classified_contents)

    # ä¿å­˜ç»“æœ
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)

    results_path = output_path / "enhanced_classification_results.json"
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(classification_report, f, indent=2, ensure_ascii=False, default=str)

    print(f"âœ… å¢å¼ºåˆ†ç±»å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {results_path}")
    print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"  - æ€»å†…å®¹æ•°: {len(classified_contents)}")
    print(f"  - å†²çªæ£€æµ‹: {sum(1 for c in classified_contents if c.conflict_detection.has_conflicts)}")
    print(f"  - å¹³å‡è´¨é‡è¯„åˆ†: {sum(c.quality_metrics.overall_quality for c in classified_contents) / len(classified_contents):.2f}")

    return classified_contents

def generate_enhanced_classification_report(contents: List[ClassifiedContent]) -> Dict:
    """ç”Ÿæˆå¢å¼ºåˆ†ç±»æŠ¥å‘Š"""
    report = {
        "classification_summary": {
            "total_contents": len(contents),
            "average_confidence": sum(c.confidence_score for c in contents) / len(contents),
            "average_quality": sum(c.quality_metrics.overall_quality for c in contents) / len(contents),
            "conflicts_detected": sum(1 for c in contents if c.conflict_detection.has_conflicts),
            "categories": {},
            "content_types": {},
            "priorities": {},
            "quality_distribution": {
                "high": sum(1 for c in contents if c.quality_metrics.overall_quality > 0.8),
                "medium": sum(1 for c in contents if 0.5 < c.quality_metrics.overall_quality <= 0.8),
                "low": sum(1 for c in contents if c.quality_metrics.overall_quality <= 0.5)
            }
        },
        "classified_contents": [],
        "skill_seekers_insights": {
            "top_conflicts": [],
            "quality_issues": [],
            "classification_accuracy": 0.0,
            "semantic_clusters": []
        }
    }

    # ç»Ÿè®¡ä¿¡æ¯
    for content in contents:
        # åŸºç¡€ç»Ÿè®¡
        cat = content.tax_category.value
        report["classification_summary"]["categories"][cat] = \
            report["classification_summary"]["categories"].get(cat, 0) + 1

        ctype = content.content_type.value
        report["classification_summary"]["content_types"][ctype] = \
            report["classification_summary"]["content_types"].get(ctype, 0) + 1

        priority = content.priority.name
        report["classification_summary"]["priorities"][priority] = \
            report["classification_summary"]["priorities"].get(priority, 0) + 1

        # è¯¦ç»†å†…å®¹
        content_dict = {
            "content_id": content.content_id,
            "tax_category": content.tax_category.value,
            "content_type": content.content_type.value,
            "priority": content.priority.name,
            "confidence_score": content.confidence_score,
            "quality_metrics": {
                "overall_quality": content.quality_metrics.overall_quality,
                "completeness_score": content.quality_metrics.completeness_score,
                "accuracy_score": content.quality_metrics.accuracy_score,
                "relevance_score": content.quality_metrics.relevance_score,
                "freshness_score": content.quality_metrics.freshness_score,
                "clarity_score": content.quality_metrics.clarity_score
            },
            "conflict_detection": {
                "has_conflicts": content.conflict_detection.has_conflicts,
                "conflict_type": content.conflict_detection.conflict_type,
                "severity": content.conflict_detection.severity,
                "description": content.conflict_detection.conflict_description
            },
            "keywords": content.keywords,
            "cross_references": content.cross_references,
            "summary": content.summary,
            "target_audience": content.target_audience,
            "semantic_similarity": content.semantic_similarity,
            "classification_path": content.classification_path,
            "verification_status": content.verification_status
        }
        report["classified_contents"].append(content_dict)

        # å†²çªæ´å¯Ÿ
        if content.conflict_detection.has_conflicts:
            report["skill_seekers_insights"]["top_conflicts"].append({
                "content_id": content.content_id,
                "conflict_type": content.conflict_detection.conflict_type,
                "severity": content.conflict_detection.severity,
                "description": content.conflict_detection.conflict_description
            })

        # è´¨é‡é—®é¢˜æ´å¯Ÿ
        if content.quality_metrics.overall_quality < 0.5:
            report["skill_seekers_insights"]["quality_issues"].append({
                "content_id": content.content_id,
                "quality_score": content.quality_metrics.overall_quality,
                "main_issues": [
                    "completeness" if content.quality_metrics.completeness_score < 0.5 else None,
                    "accuracy" if content.quality_metrics.accuracy_score < 0.5 else None,
                    "relevance" if content.quality_metrics.relevance_score < 0.5 else None,
                    "clarity" if content.quality_metrics.clarity_score < 0.5 else None
                ]
            })

    # è®¡ç®—åˆ†ç±»å‡†ç¡®æ€§
    high_confidence_contents = sum(1 for c in contents if c.confidence_score > 0.8)
    report["skill_seekers_insights"]["classification_accuracy"] = high_confidence_contents / len(contents)

    return report

if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    with open("t4012_extracted.json", 'r', encoding='utf-8') as f:
        extracted_data = json.load(f)

    classified = classify_cra_document_enhanced(extracted_data)
```

## Skill_Seekers æŠ€æœ¯é›†æˆæ€»ç»“

### æ ¸å¿ƒå¢å¼ºç‰¹æ€§

1. **æ™ºèƒ½åˆ†ç±»ç®—æ³•**
   - TF-IDF å‘é‡åŒ–åˆ†æ
   - è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
   - å¤šå› å­è¯„åˆ†æœºåˆ¶
   - åŠ¨æ€ç½®ä¿¡åº¦è°ƒæ•´

2. **å†²çªæ£€æµ‹ç³»ç»Ÿ**
   - æ•°å€¼å†²çªè¯†åˆ«
   - æ—¶é—´ç›¸å…³æ€§åˆ†æ
   - å†…å®¹ä¸€è‡´æ€§éªŒè¯
   - ä¸¥é‡ç¨‹åº¦è¯„ä¼°

3. **è´¨é‡è¯„ä¼°æ¡†æ¶**
   - äº”ç»´åº¦è´¨é‡è¯„åˆ†
   - ç»¼åˆè´¨é‡è®¡ç®—
   - è´¨é‡é—®é¢˜è¯†åˆ«
   - æ”¹è¿›å»ºè®®ç”Ÿæˆ

4. **æ™ºèƒ½äº¤å‰å¼•ç”¨**
   - è¯­ä¹‰å…³è”åˆ†æ
   - å¤šå±‚æ¬¡å¼•ç”¨å»ºç«‹
   - ç›¸å…³æ€§è¯„åˆ†
   - å¼•ç”¨è´¨é‡ä¼˜åŒ–

### ä¸åŸæœ‰ç³»ç»Ÿçš„å…¼å®¹æ€§

- ä¿æŒåŸæœ‰åˆ†ç±»æ¥å£
- æ‰©å±•æ•°æ®ç»“æ„æ”¯æŒ
- å‘åå…¼å®¹ç°æœ‰æ ¼å¼
- æ¸è¿›å¼å‡çº§è·¯å¾„

### æ€§èƒ½ä¼˜åŒ–

- å‘é‡åŒ–è®¡ç®—ä¼˜åŒ–
- æ‰¹é‡å¤„ç†æ”¯æŒ
- å†…å­˜ä½¿ç”¨ä¼˜åŒ–
- ç¼“å­˜æœºåˆ¶é›†æˆ

## æµ‹è¯•éªŒè¯

### Skill_Seekers ç‰¹æ€§æµ‹è¯•

```python
def test_enhanced_classification():
    """æµ‹è¯•å¢å¼ºåˆ†ç±»åŠŸèƒ½"""
    classifier = EnhancedTaxContentClassifier()

    # æµ‹è¯•è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
    semantic_scores = classifier._calculate_semantic_similarity(
        "This is about capital gains tax rates"
    )
    assert isinstance(semantic_scores, dict)

    # æµ‹è¯•å†²çªæ£€æµ‹
    test_contents = [create_test_content_with_conflict()]
    classifier._detect_conflicts(test_contents)
    assert test_contents[0].conflict_detection.has_conflicts

    # æµ‹è¯•è´¨é‡è¯„ä¼°
    classifier._assess_content_quality(test_contents)
    assert test_contents[0].quality_metrics.overall_quality > 0

def test_skill_seekers_integration():
    """æµ‹è¯• Skill_Seekers é›†æˆ"""
    # æ¨¡æ‹Ÿ CRA æ•°æ®
    cra_data = create_mock_cra_data()

    result = classify_cra_document_enhanced(cra_data)

    # éªŒè¯å¢å¼ºç‰¹æ€§
    assert len(result) > 0
    assert all(hasattr(c, 'quality_metrics') for c in result)
    assert all(hasattr(c, 'conflict_detection') for c in result)
    assert all(hasattr(c, 'semantic_similarity') for c in result)
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
def benchmark_enhanced_vs_original():
    """å¯¹æ¯”å¢å¼ºç‰ˆä¸åŸç‰ˆçš„æ€§èƒ½"""
    import time

    # æµ‹è¯•æ•°æ®
    test_data = generate_large_test_dataset(1000)

    # åŸç‰ˆåˆ†ç±»å™¨
    start_time = time.time()
    original_results = original_classify_cra_document(test_data)
    original_time = time.time() - start_time

    # å¢å¼ºç‰ˆåˆ†ç±»å™¨
    start_time = time.time()
    enhanced_results = classify_cra_document_enhanced(test_data)
    enhanced_time = time.time() - start_time

    print(f"åŸç‰ˆè€—æ—¶: {original_time:.2f}s")
    print(f"å¢å¼ºç‰ˆè€—æ—¶: {enhanced_time:.2f}s")
    print(f"æ€§èƒ½æ¯”: {enhanced_time/original_time:.2f}x")

    # è´¨é‡å¯¹æ¯”
    original_avg_conf = sum(c.confidence_score for c in original_results) / len(original_results)
    enhanced_avg_conf = sum(c.confidence_score for c in enhanced_results) / len(enhanced_results)

    print(f"åŸç‰ˆå¹³å‡ç½®ä¿¡åº¦: {original_avg_conf:.2f}")
    print(f"å¢å¼ºç‰ˆå¹³å‡ç½®ä¿¡åº¦: {enhanced_avg_conf:.2f}")
    print(f"è´¨é‡æå‡: {((enhanced_avg_conf - original_avg_conf) / original_avg_conf * 100):.1f}%")
```

## ä¾èµ–å…³ç³»æ›´æ–°

**æ–°å¢ä¾èµ–ï¼š**
```toml
# Skill_Seekers é›†æˆä¾èµ–
numpy>=1.24.0              # æ•°å€¼è®¡ç®—
scikit-learn>=1.3.0        # æœºå™¨å­¦ä¹ ç®—æ³•
```

**å‡çº§ä¾èµ–ï¼š**
```toml
# å¢å¼ºç‰ˆä¾èµ–
nltk>=3.8.0                # å¢å¼ºè‡ªç„¶è¯­è¨€å¤„ç†
spacy>=3.7.0               # é«˜çº§å®ä½“è¯†åˆ«ï¼ˆå¯é€‰ï¼‰
```

**å‰ç½®ä»»åŠ¡ï¼š**
- ä»»åŠ¡05ï¼šPDF æ–‡æœ¬æå–æ¨¡å—

**åç½®ä»»åŠ¡ï¼š**
- ä»»åŠ¡07ï¼šSkill ç”Ÿæˆæ¨¡å—ï¼ˆä½¿ç”¨å¢å¼ºçš„åˆ†ç±»ç»“æœï¼‰

è¿™ä¸ªå¢å¼ºç‰ˆå†…å®¹åˆ†ç±»æ¨¡å—é›†æˆäº† Skill_Seekers çš„å…ˆè¿›æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº† CRA æ–‡æ¡£å¤„ç†çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œä¸ºç”Ÿæˆé«˜è´¨é‡çš„ç¨åŠ¡çŸ¥è¯†åº“å¥ å®šäº†åšå®åŸºç¡€ã€‚
