# ç¼“å­˜ç®¡ç†

BeanFlow-CRA ä½¿ç”¨æ–‡ä»¶ç¼“å­˜åŠ é€Ÿé‡å¤å¤„ç†ã€‚æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜ç¼“å­˜æœºåˆ¶ã€ç®¡ç†æ–¹æ³•å’Œæœ€ä½³å®è·µã€‚

---

## ç¼“å­˜æœºåˆ¶æ¦‚è¿°

### è®¾è®¡ç›®æ ‡

| ç›®æ ‡ | è¯´æ˜ |
|------|------|
| **åŠ é€Ÿé‡å¤„ç†** | è·³è¿‡å·²å®Œæˆé˜¶æ®µï¼Œé‡æ–°è¿è¡Œæ—¶èŠ‚çœæ—¶é—´ |
| **æ–­ç‚¹ç»­ä¼ ** | ä¸­æ–­åå¯ä»ä¸Šæ¬¡ä½ç½®ç»§ç»­ |
| **è°ƒè¯•æ”¯æŒ** | æ£€æŸ¥å„é˜¶æ®µä¸­é—´ç»“æœ |
| **æ•…éšœæ¢å¤** | ä»…é‡è¯•å¤±è´¥éƒ¨åˆ†ï¼Œæ— éœ€å…¨éƒ¨é‡æ¥ |

### å·¥ä½œåŸç†

```
PDF æ–‡ä»¶ â†’ SHA256 Hashï¼ˆå‰ 16 ä½ï¼‰â†’ ä½œä¸ºç¼“å­˜é”®

è¿è¡Œæ—¶æµç¨‹:
1. è®¡ç®— PDF æ–‡ä»¶ hash
2. æ£€æŸ¥å„é˜¶æ®µç¼“å­˜æ˜¯å¦å­˜åœ¨
3. è·³è¿‡å·²ç¼“å­˜é˜¶æ®µï¼Œæ‰§è¡Œæœªå®Œæˆé˜¶æ®µ
4. ä¿å­˜æ–°ç»“æœåˆ°ç¼“å­˜
```

### Hash è®¡ç®—æ–¹å¼

ç³»ç»Ÿä½¿ç”¨ PDF æ–‡ä»¶å†…å®¹çš„ SHA256 å“ˆå¸Œï¼ˆå‰ 16 ä½ï¼‰ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼š

```python
def hash_file(file_path: Path) -> str:
    """è®¡ç®—æ–‡ä»¶ SHA256 hashï¼ˆå‰16ä½ï¼‰"""
    sha256 = hashlib.sha256()
    with open(file_path, 'rb') as f:
        while chunk := f.read(8192):
            sha256.update(chunk)
    return sha256.hexdigest()[:16]
```

**ç‰¹æ€§**:
- åŒä¸€ PDF æ–‡ä»¶ â†’ ç›¸åŒ hash â†’ å¤ç”¨ç¼“å­˜
- PDF å†…å®¹å˜åŒ– â†’ æ–° hash â†’ æ–°ç¼“å­˜
- è‡ªåŠ¨ç¼“å­˜å¤±æ•ˆæœºåˆ¶

---

## ç¼“å­˜ç›®å½•ç»“æ„

### ç›®å½•ä½ç½®

é»˜è®¤ç¼“å­˜ç›®å½•: `backend/cache/`

å¯é€šè¿‡ `--cache-dir` å‚æ•°è‡ªå®šä¹‰ï¼š
```bash
uv run python generate_skill.py --pdf file.pdf --cache-dir /custom/cache
```

### æ–‡ä»¶ç»“æ„

```
backend/cache/
â”œâ”€â”€ README.md                           # ç¼“å­˜è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ extraction_<pdf-hash>.json          # Stage 1: PDF æå–ç»“æœ
â”œâ”€â”€ classification_<pdf-hash>.json      # Stage 2: åˆ†ç±»ç»“æœ
â”œâ”€â”€ chunks_<pdf-hash>.json              # Stage 3: åˆ†å—ç»“æœ
â””â”€â”€ enhanced_chunks_<pdf-hash>/         # Stage 4: AI å¢å¼ºç»“æœ
    â”œâ”€â”€ progress.json                   # è¿›åº¦è¿½è¸ª
    â”œâ”€â”€ chunk-001.json                  # å¢å¼ºåçš„ chunk 1
    â”œâ”€â”€ chunk-002.json                  # å¢å¼ºåçš„ chunk 2
    â””â”€â”€ ...                             # æ›´å¤š chunks
```

---

## ç¼“å­˜æ–‡ä»¶æ ¼å¼è¯¦è§£

### Stage 1: extraction_&lt;hash&gt;.json

PDF æ–‡æœ¬æå–ç»“æœï¼š

```json
{
  "stage": "extraction",
  "content_hash": "abc123def456789",
  "timestamp": "2025-11-04T10:00:00",
  "metadata": {
    "pdf_path": "../mvp/pdf/t4012-24e.pdf",
    "total_pages": 151
  },
  "data": {
    "pdf_path": "../mvp/pdf/t4012-24e.pdf",
    "pdf_hash": "abc123def456789",
    "total_pages": 151,
    "total_text": "å®Œæ•´æå–æ–‡æœ¬...",
    "pages": [
      {
        "page_number": 1,
        "text": "ç¬¬1é¡µæ–‡æœ¬...",
        "char_count": 3500,
        "line_count": 85
      }
    ],
    "extraction_time": "2025-11-04T10:00:00"
  }
}
```

| å­—æ®µ | è¯´æ˜ |
|------|------|
| `stage` | é˜¶æ®µæ ‡è¯† |
| `content_hash` | PDF æ–‡ä»¶ hash |
| `timestamp` | åˆ›å»ºæ—¶é—´ |
| `metadata.total_pages` | æ€»é¡µæ•° |
| `data.total_text` | å®Œæ•´æå–æ–‡æœ¬ |
| `data.pages` | åˆ†é¡µæå–ç»“æœ |

---

### Stage 2: classification_&lt;hash&gt;.json

å†…å®¹åˆ†ç±»ç»“æœï¼š

```json
{
  "stage": "classification",
  "content_hash": "abc123def456789",
  "timestamp": "2025-11-04T10:02:00",
  "data": {
    "primary_category": "employment_income",
    "confidence": 0.85,
    "secondary_categories": [
      {"category": "deductions", "confidence": 0.45},
      {"category": "credits", "confidence": 0.30}
    ],
    "quality_metrics": {
      "keyword_coverage": 0.78,
      "structure_quality": 0.82,
      "content_depth": 0.75,
      "specificity_score": 0.88,
      "completeness_score": 0.70
    },
    "matched_keywords": ["T4", "employment", "income", "withholding"]
  }
}
```

| å­—æ®µ | è¯´æ˜ |
|------|------|
| `primary_category` | ä¸»åˆ†ç±» |
| `confidence` | ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰ |
| `secondary_categories` | æ¬¡è¦åˆ†ç±»åˆ—è¡¨ |
| `quality_metrics` | è´¨é‡è¯„åˆ†è¯¦æƒ… |
| `matched_keywords` | åŒ¹é…çš„å…³é”®è¯ |

---

### Stage 3: chunks_&lt;hash&gt;.json

å†…å®¹åˆ†å—ç»“æœï¼š

```json
{
  "stage": "chunking",
  "content_hash": "abc123def456789",
  "timestamp": "2025-11-04T10:02:10",
  "data": {
    "total_chunks": 83,
    "chunks": [
      {
        "chunk_id": 1,
        "chapter_num": 1,
        "title": "Chapter 1: Page 1 of T2 return",
        "slug": "chapter-1-page-1-of-t2-return",
        "content": "ç« èŠ‚å†…å®¹...",
        "char_count": 8500
      }
    ],
    "chunking_strategy": "chapter_detection",
    "avg_chunk_size": 8693,
    "max_chunk_size": 295000,
    "min_chunk_size": 1200
  }
}
```

| å­—æ®µ | è¯´æ˜ |
|------|------|
| `total_chunks` | æ€» chunk æ•°é‡ |
| `chunks` | chunk åˆ—è¡¨ |
| `chunking_strategy` | åˆ†å—ç­–ç•¥ |
| `avg_chunk_size` | å¹³å‡ chunk å¤§å° |

---

### Stage 4: enhanced_chunks_&lt;hash&gt;/

#### progress.json - è¿›åº¦è¿½è¸ª

```json
{
  "total_chunks": 83,
  "completed_chunks": 40,
  "failed_chunks": [15, 23],
  "start_time": "2025-11-04T10:05:00",
  "last_update": "2025-11-04T13:30:00",
  "estimated_remaining": "180 minutes",
  "provider": "glm-api",
  "avg_processing_time": 4.5
}
```

| å­—æ®µ | è¯´æ˜ |
|------|------|
| `total_chunks` | æ€» chunk æ•° |
| `completed_chunks` | å·²å®Œæˆæ•° |
| `failed_chunks` | å¤±è´¥çš„ chunk ID åˆ—è¡¨ |
| `provider` | ä½¿ç”¨çš„ LLM Provider |
| `avg_processing_time` | å¹³å‡å¤„ç†æ—¶é—´ï¼ˆåˆ†é’Ÿ/chunkï¼‰ |

#### chunk-XXX.json - å¢å¼ºåçš„ chunk

```json
{
  "chunk_id": 1,
  "original_title": "Chapter 1: Page 1 of T2 return",
  "slug": "chapter-1-page-1-of-t2-return",
  "enhanced_content": "# Chapter 1: Page 1 of T2 return\n\nå¢å¼ºåçš„å†…å®¹...",
  "enhancement_time": "2025-11-04T10:10:00",
  "provider": "glm-api",
  "status": "completed",
  "token_count": 75000,
  "processing_duration": 245
}
```

| å­—æ®µ | è¯´æ˜ |
|------|------|
| `enhanced_content` | å¢å¼ºåçš„ Markdown å†…å®¹ |
| `status` | çŠ¶æ€: completed/failed |
| `token_count` | è¾“å‡º token æ•° |
| `processing_duration` | å¤„ç†è€—æ—¶ï¼ˆç§’ï¼‰ |

---

## ç¼“å­˜æ–‡ä»¶å¤§å°å‚è€ƒ

### å• PDF ç¼“å­˜å¤§å°

| æ–‡ä»¶ç±»å‹ | å…¸å‹å¤§å° | è¯´æ˜ |
|----------|----------|------|
| `extraction_*.json` | 500 KB - 2 MB | å–å†³äº PDF é¡µæ•°å’Œæ–‡æœ¬é‡ |
| `classification_*.json` | 5 - 50 KB | åˆ†ç±»ç»“æœè¾ƒå° |
| `chunks_*.json` | 500 KB - 2 MB | ä¸ extraction ç›¸è¿‘ |
| `enhanced_chunks_*/` | 5 - 20 MB | æ¯ä¸ª chunk 60-240 KB |

### å®é™…æ¡ˆä¾‹ï¼ˆ151 é¡µ PDFï¼‰

| æ–‡ä»¶ | å¤§å° |
|------|------|
| extraction_xxx.json | ~1.5 MB |
| classification_xxx.json | ~10 KB |
| chunks_xxx.json | ~1.5 MB |
| enhanced_chunks_xxx/ | ~15 MB (83 chunks) |
| **æ€»è®¡** | **~18 MB** |

### ä¼°ç®—å…¬å¼

```
å• PDF ç¼“å­˜ â‰ˆ (é¡µæ•° Ã— 10 KB) + (chunkæ•° Ã— 180 KB)

ä¾‹å¦‚ 151 é¡µ PDF:
= (151 Ã— 10 KB) + (83 Ã— 180 KB)
= 1.5 MB + 15 MB
â‰ˆ 16.5 MB
```

---

## ç¼“å­˜ç®¡ç†å‘½ä»¤

### æŸ¥çœ‹ç¼“å­˜çŠ¶æ€

```bash
# æŸ¥çœ‹ç¼“å­˜ç›®å½•å¤§å°
du -sh backend/cache/

# æŸ¥çœ‹å„æ–‡ä»¶å¤§å°
du -sh backend/cache/*

# åˆ—å‡ºæ‰€æœ‰ç¼“å­˜çš„ PDF hash
ls backend/cache/extraction_*.json | sed 's/.*extraction_\(.*\)\.json/\1/'

# æŸ¥çœ‹ç‰¹å®š hash çš„æ‰€æœ‰ç¼“å­˜
ls -la backend/cache/*abc123*
```

### æŸ¥çœ‹å¤„ç†è¿›åº¦

```bash
# æŸ¥çœ‹ Stage 4 è¿›åº¦
cat backend/cache/enhanced_chunks_*/progress.json | python -m json.tool

# æŸ¥çœ‹å·²å®Œæˆçš„ chunks
ls backend/cache/enhanced_chunks_*/chunk-*.json | wc -l

# æŸ¥çœ‹å¤±è´¥çš„ chunks
jq '.failed_chunks' backend/cache/enhanced_chunks_*/progress.json
```

### æ¸…ç†ç¼“å­˜

#### æ–¹æ³• 1: å‘½ä»¤è¡Œæ¸…ç†

```bash
# åˆ é™¤æ‰€æœ‰ç¼“å­˜ï¼ˆä¿ç•™ READMEï¼‰
rm -rf backend/cache/*
git checkout backend/cache/README.md

# åˆ é™¤ç‰¹å®š hash çš„ç¼“å­˜
rm backend/cache/*_abc123*
rm -rf backend/cache/enhanced_chunks_abc123/

# åˆ é™¤ 7 å¤©å‰çš„ç¼“å­˜
find backend/cache -name "*.json" -mtime +7 -delete
find backend/cache -type d -name "enhanced_chunks_*" -mtime +7 -exec rm -rf {} \;

# åªåˆ é™¤ Stage 4 ç¼“å­˜ï¼ˆä¿ç•™ Stage 1-3ï¼‰
rm -rf backend/cache/enhanced_chunks_*/
```

#### æ–¹æ³• 2: Python API æ¸…ç†

```python
from app.document_processor.pipeline_manager import CacheManager

cache_mgr = CacheManager()

# æ¸…ç† 7 å¤©å‰çš„ç¼“å­˜
cache_mgr.clean_cache(older_than_days=7)

# æ¸…ç†ç‰¹å®š hash çš„ç¼“å­˜
cache_mgr.clean_cache(content_hash="abc123def456789")

# åˆ—å‡ºæ‰€æœ‰ç¼“å­˜çš„ PDFs
pdfs = cache_mgr.list_cached_pdfs()
for pdf in pdfs:
    print(f"{pdf['content_hash']}: {pdf['pdf_path']}")
```

### å¼ºåˆ¶é‡æ–°å¤„ç†

```bash
# å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰é˜¶æ®µ
uv run python generate_skill.py --pdf file.pdf --glm-api --force

# åªå¼ºåˆ¶é‡æ–°æå–ï¼ˆStage 1ï¼‰
uv run python generate_skill.py --pdf file.pdf --glm-api --force-extract

# é‡è¯•å¤±è´¥çš„ chunksï¼ˆStage 4ï¼‰
uv run python stage4_enhance_chunks.py --chunks-id <hash> --retry-failed --provider glm-api
```

---

## æ–­ç‚¹ç»­ä¼ æœºåˆ¶

### å·¥ä½œåŸç†

Stage 4 æ”¯æŒä¸­æ–­åè‡ªåŠ¨æ¢å¤ï¼š

```
é¦–æ¬¡è¿è¡Œ:
Chunk 1 â†’ å®Œæˆ â†’ ä¿å­˜
Chunk 2 â†’ å®Œæˆ â†’ ä¿å­˜
Chunk 3 â†’ å®Œæˆ â†’ ä¿å­˜
[Ctrl+C ä¸­æ–­]

å†æ¬¡è¿è¡Œ:
è¯»å– progress.json â†’ completed_chunks=3
ä» Chunk 4 ç»§ç»­ â†’ ...
```

### ä½¿ç”¨æ–¹æ³•

```bash
# é¦–æ¬¡è¿è¡Œ
uv run python generate_skill.py --pdf file.pdf --glm-api --full
# æŒ‰ Ctrl+C ä¸­æ–­

# æ¢å¤è¿è¡Œï¼ˆè‡ªåŠ¨æ£€æµ‹å¹¶ç»­ä¼ ï¼‰
uv run python generate_skill.py --pdf file.pdf --glm-api --full
# è¾“å‡º: ğŸ’¡ Detected incomplete enhancement (40/83 chunks)
#       Will resume from chunk 41
```

### æ‰‹åŠ¨æ§åˆ¶

```bash
# æ˜ç¡®æŒ‡å®šç»­ä¼ 
uv run python stage4_enhance_chunks.py --chunks-id <hash> --resume --provider glm-api

# ä»å¤´å¼€å§‹ï¼ˆåˆ é™¤è¿›åº¦åé‡æ–°å¤„ç†ï¼‰
rm backend/cache/enhanced_chunks_<hash>/progress.json
uv run python stage4_enhance_chunks.py --chunks-id <hash> --provider glm-api
```

---

## ç¼“å­˜å¤±æ•ˆæ¡ä»¶

### è‡ªåŠ¨å¤±æ•ˆ

| æ¡ä»¶ | å½±å“é˜¶æ®µ | è¯´æ˜ |
|------|---------|------|
| PDF å†…å®¹å˜åŒ– | æ‰€æœ‰é˜¶æ®µ | æ–° hashï¼Œå…¨éƒ¨é‡æ–°å¤„ç† |
| `--max-pages` æ”¹å˜ | Stage 1+ | æå–é¡µæ•°ä¸åŒï¼Œéœ€é‡æ–°å¤„ç† |
| `--force` å‚æ•° | æ‰€æœ‰é˜¶æ®µ | å¼ºåˆ¶å¿½ç•¥ç¼“å­˜ |
| `--force-extract` å‚æ•° | Stage 1+ | å¼ºåˆ¶é‡æ–°æå– |

### æ‰‹åŠ¨å¤±æ•ˆ

```bash
# åˆ é™¤ç‰¹å®šé˜¶æ®µç¼“å­˜ï¼Œè§¦å‘é‡æ–°å¤„ç†
rm backend/cache/classification_<hash>.json  # é‡æ–°åˆ†ç±»
rm backend/cache/chunks_<hash>.json          # é‡æ–°åˆ†å—
rm -rf backend/cache/enhanced_chunks_<hash>/ # é‡æ–°å¢å¼º
```

---

## æœ€ä½³å®è·µ

### å¼€å‘æœŸé—´

1. **ä¿ç•™ç¼“å­˜**: åŠ é€Ÿè¿­ä»£ï¼Œé¿å…é‡å¤å¤„ç†
2. **å–„ç”¨ `--no-ai`**: æµ‹è¯• Stage 1-3 æ—¶è·³è¿‡è€—æ—¶çš„ Stage 4
3. **ä½¿ç”¨å°é¡µæ•°æµ‹è¯•**: `--max-pages 10` å¿«é€ŸéªŒè¯

```bash
# å¼€å‘æµ‹è¯•å‘½ä»¤
uv run python generate_skill.py --pdf file.pdf --no-ai --max-pages 10
```

### ç”Ÿäº§ç¯å¢ƒ

1. **å®šæœŸæ¸…ç†**: é¿å…ç£ç›˜ç©ºé—´è€—å°½
2. **ç›‘æ§å¤§å°**: è®¾ç½®å‘Šè­¦é˜ˆå€¼
3. **å¤‡ä»½é‡è¦ç¼“å­˜**: é•¿æ—¶é—´å¤„ç†çš„ç»“æœ

```bash
# å®šæœŸæ¸…ç†è„šæœ¬ï¼ˆå¯åŠ å…¥ cronï¼‰
#!/bin/bash
# æ¸…ç† 7 å¤©å‰çš„ç¼“å­˜
find /path/to/backend/cache -name "*.json" -mtime +7 -delete
find /path/to/backend/cache -type d -name "enhanced_chunks_*" -mtime +7 -exec rm -rf {} \;

# å‘é€å‘Šè­¦ï¼ˆå¦‚ç£ç›˜ä½¿ç”¨è¶…è¿‡ 80%ï¼‰
USAGE=$(du -s /path/to/backend/cache | cut -f1)
if [ $USAGE -gt 10000000 ]; then  # 10GB
    echo "Cache size warning: ${USAGE}KB"
fi
```

### æ‰¹é‡å¤„ç†

```bash
# æ‰¹é‡å¤„ç†å¤šä¸ª PDF
for pdf in pdfs/*.pdf; do
    echo "Processing: $pdf"
    uv run python generate_skill.py --pdf "$pdf" --glm-api --full
done

# ç›‘æ§æ€»ç¼“å­˜å¤§å°
watch -n 60 'du -sh backend/cache/'
```

---

## æ³¨æ„äº‹é¡¹

### ä¸è¦æ‰‹åŠ¨ç¼–è¾‘ç¼“å­˜

ç¼“å­˜æ–‡ä»¶ç”±ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆå’Œç®¡ç†ã€‚æ‰‹åŠ¨ç¼–è¾‘å¯èƒ½å¯¼è‡´ï¼š
- æ•°æ®ä¸ä¸€è‡´
- å¤„ç†å¤±è´¥
- æ— æ³•æ–­ç‚¹ç»­ä¼ 

### ç¼“å­˜ä¸çº³å…¥ç‰ˆæœ¬æ§åˆ¶

`.gitignore` å·²é…ç½®æ’é™¤ç¼“å­˜ç›®å½•ï¼š

```gitignore
backend/cache/
!backend/cache/README.md
```

### è·¨æœºå™¨ç¼“å­˜

ç¼“å­˜æ–‡ä»¶å¯ä»¥è·¨æœºå™¨å¤åˆ¶ä½¿ç”¨ï¼ˆå¦‚æœ PDF æ–‡ä»¶ç›¸åŒï¼‰ï¼Œä½†éœ€æ³¨æ„ï¼š
- ç¡®ä¿ `pdf_path` åœ¨ç›®æ ‡æœºå™¨æœ‰æ•ˆ
- Stage 4 ä½¿ç”¨çš„ Provider éœ€ä¸€è‡´

---

## ç›¸å…³æ–‡æ¡£

- [Pipeline æ¶æ„](../architecture/PIPELINE_ARCHITECTURE.md) - è¯¦ç»†çš„ç¼“å­˜è®¾è®¡è¯´æ˜
- [æ•…éšœæ’æŸ¥](TROUBLESHOOTING.md) - ç¼“å­˜ç›¸å…³é—®é¢˜è§£å†³
- [backend/cache/README.md](../../backend/cache/README.md) - ç¼“å­˜ç›®å½•è¯´æ˜

---

**ç‰ˆæœ¬**: 2.0
**æ›´æ–°**: 2025-12-08
