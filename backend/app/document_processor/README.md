# Document Processor - Phase-02

Offline CRA tax document processing modules for generating high-quality Skill files.

## Quick Start

### Process Complete PDF with Local LLM CLI

**Option 1: Claude Code CLI** (recommended if you have Claude Code subscription)

```bash
cd backend
uv run python generate_skill.py --pdf ../mvp/pdf/t4012-24e.pdf --local-claude --full
```

**Option 2: Gemini CLI** (free tier: 1000 req/day)

```bash
cd backend
uv run python generate_skill.py --pdf ../mvp/pdf/t4012-24e.pdf --local-gemini --full
```

**Option 3: OpenAI Codex CLI** (requires OpenAI access)

```bash
cd backend
uv run python generate_skill.py --pdf ../mvp/pdf/t4012-24e.pdf --local-codex --full
```

This will:
- Extract all 153 pages from the PDF (721K characters)
- Detect chapters and split into ~3 intelligent chunks
- Classify content by tax category
- Enhance each chunk using selected local LLM CLI
- Generate skill directory with:
  - `SKILL.md` - Lightweight index (< 500 lines)
  - `references/*.md` - AI-enhanced chapters (3 files)
  - `references/index.md` - Navigation
  - `raw/full-extract.txt` - Complete original text
- Save to `skills_output/credits-t4012-24e/`

**Processing Time** (full 153-page PDF, 721K chars):
- **Claude**: ~15-25 minutes (3 chunks Ã— 5-8 min/chunk)
- **Gemini**: ~5-10 minutes (1 chunk, 5x larger context window)
- **Codex**: ~15-20 minutes (3 chunks Ã— 5-7 min/chunk)

### Quick Test (First 5 Pages)

```bash
cd backend
uv run python generate_skill.py --pdf ../mvp/pdf/t4012-24e.pdf --glm-api --max-pages 5
```

---

## Modules Overview

### âœ… All Modules Completed (English-only, fully typed)

1. **pdf_extractor.py** (280 lines) - PDF text extraction
   - PyMuPDF-based extraction
   - OCR support with pytesseract
   - Intelligent quality assessment
   - Complete type annotations
   - Pydantic models: `PDFExtractorConfig`, `PageResult`, `PDFMetadata`, `ExtractionResult`

2. **content_classifier.py** - Quality assessment models
   - 5-dimensional quality metrics (completeness, accuracy, relevance, clarity, practicality)
   - Classification result models
   - Pydantic models: `QualityMetrics`, `ClassificationResult`
   - Note: TaxCategory enum removed - categories are now dynamically generated by LLM

3. **dynamic_classifier.py** - LLM-based dynamic semantic classification
   - Uses LLM for intelligent document classification
   - Generates categories dynamically based on document content
   - No predefined category vocabulary required

4. **skill_generator.py** (280 lines) - Skill file generation
   - Generate Markdown with YAML front matter
   - Compatible with MVP skill_loader.py
   - Hierarchical organization by category
   - Auto-generate IDs, titles, tags, descriptions
   - Pydantic models: `SkillMetadata`, `SkillContent`

4. **markdown_optimizer.py** (250 lines) - AI content enhancement
   - Basic cleaning (PDF artifacts, formatting)
   - AI enhancement with Claude/OpenAI/ZhipuAI APIs
   - Offline processing (API usage allowed)
   - Pydantic models: `OptimizationConfig`, `OptimizationResult`, `AIProvider`

5. **quality_validator.py** (320 lines) - Quality validation
   - YAML front matter validation
   - Markdown format checking
   - Content quality assessment
   - Tax content accuracy validation
   - Pydantic models: `ValidationIssue`, `ValidationResult`

6. **__init__.py** (95 lines) - Module initialization
   - Clean exports
   - Version information
   - All public APIs exposed

7. **tests/test_document_processing.py** (270 lines) - Integration tests
   - âœ… All 6 tests passing
   - English-only test output
   - Full pipeline validation

---

## Code Standards Compliance

All code follows project standards from `.claude/skills/development-policies`:

- âœ… All identifiers in English
- âœ… All docstrings in English
- âœ… All Field descriptions in English
- âœ… Complete type annotations
- âœ… Pydantic models for data validation
- âœ… Structured logging
- âœ… Proper error handling

---

## Usage

### Python API

```python
from app.document_processor import (
    PDFTextExtractor,
    ContentClassifier,
    SkillGenerator,
    MarkdownOptimizer,
    QualityValidator
)

# 1. Extract PDF text
extractor = PDFTextExtractor()
extraction_result = extractor.extract("document.pdf")

# 2. Classify content
classifier = ContentClassifier()
classification = classifier.classify(
    extraction_result.total_text,
    title="Tax Document"
)

# 3. Generate Skill file
generator = SkillGenerator(output_dir="skills")
skill = generator.generate_skill(
    content=extraction_result.total_text,
    classification=classification,
    source_file="document.pdf"
)

# 4. Optimize with AI (optional)
optimizer = MarkdownOptimizer()
optimized = optimizer.optimize(skill.markdown_body)
skill.markdown_body = optimized.optimized_content

# 5. Validate quality
validator = QualityValidator()
validation = validator.validate_content(
    generator._render_skill_file(skill)
)

# 6. Save if valid
if validation.is_valid:
    file_path = generator.save_skill(skill)
    print(f"Skill saved: {file_path}")
```

### CLI Script

The `generate_skill.py` script provides a convenient way to generate skills from PDFs:

**Location**: `backend/generate_skill.py`

**Command Options**:

| Flag | Type | Default | Description |
|------|------|---------|-------------|
| `--pdf` | Required | - | Path to PDF file |
| `--full` | Flag | False | **Process ALL pages** (otherwise limited to --max-pages) |
| `--local-claude` | Flag | False | Use local Claude Code CLI (requires subscription) |
| `--local-gemini` | Flag | False | Use local Gemini CLI (free tier: 1000 req/day) |
| `--local-codex` | Flag | False | Use local Codex CLI (requires OpenAI access) |
| `--glm-api` | Flag | False | Use GLM API (requires GLM_API_KEY) |
| `--api` | Flag | False | Use Claude API (requires ANTHROPIC_API_KEY) |
| `--max-pages N` | Integer | 10 | Max pages to process when not using --full |
| `--output-dir PATH` | String | `skills_output` | Output directory for generated skills |

**Example Commands**:

```bash
# Complete processing with local Claude (if you have Claude Code subscription)
uv run python generate_skill.py \
  --pdf ../mvp/pdf/t4012-24e.pdf \
  --local-claude \
  --full

# Complete processing with Gemini (free tier)
uv run python generate_skill.py \
  --pdf ../mvp/pdf/t4012-24e.pdf \
  --local-gemini \
  --full

# Complete processing with Codex
uv run python generate_skill.py \
  --pdf ../mvp/pdf/t4012-24e.pdf \
  --local-codex \
  --full

# Fast test with GLM API (first 10 pages)
uv run python generate_skill.py \
  --pdf ../mvp/pdf/t4012-24e.pdf \
  --glm-api

# Custom page range with API enhancement
export ANTHROPIC_API_KEY="sk-ant-..."
uv run python generate_skill.py \
  --pdf ../mvp/pdf/t4012-24e.pdf \
  --api \
  --max-pages 20
```

---

## LLM CLI Provider Setup

### Default Models Overview

All providers use optimized default models for best quality:

| Provider | Default Model | Context Window | Override Method |
|----------|--------------|----------------|-----------------|
| **Claude** | `sonnet` (4.5) | 200K tokens | `ANTHROPIC_MODEL=opus` |
| **Gemini** | `gemini-2.5-pro` | 1M tokens | `GEMINI_MODEL=gemini-2.5-flash` |
| **Codex** | System default | ~128K tokens | `codex exec --model gpt-5-codex` |

---

### Claude Code CLI

**Installation**: Download from [claude.com](https://claude.com/claude-code)

**Requirements**: Claude Code subscription (Code Max plan)

**Command**: `claude`

**Default Model**: `sonnet` (Claude Sonnet 4.5)
- Balanced quality and speed
- Optimal for document processing
- Override via environment: `export ANTHROPIC_MODEL=opus`

**Verification**:
```bash
which claude
# Should output: /path/to/claude
```

### Gemini CLI

**Installation**:
```bash
npm install -g @google/gemini-cli
```

**Requirements**:
- Google account
- Free tier: 1000 requests/day, 60 requests/minute
- 1M token context window

**Command**: `gemini`

**Default Model**: `gemini-2.5-pro`
- Highest quality from Gemini family
- 1M token context window
- Best for large documents
- Override via environment: `export GEMINI_MODEL=gemini-2.5-flash`

**Verification**:
```bash
which gemini
gemini --version
```

**Features**:
- Open source (Apache 2.0 license)
- Free tier available
- Faster than Claude for large contexts (1.5M char chunks)
- GitHub: `google-gemini/gemini-cli`

### OpenAI Codex CLI

**Installation**:
```bash
# Download from GitHub releases
# Visit: https://github.com/openai/codex
```

**Requirements**:
- ChatGPT subscription or OpenAI API key
- macOS or Linux (Windows via WSL)

**Authentication**:
```bash
# Interactive login
codex login

# Or use environment variable
export CODEX_API_KEY="your-api-key"
```

**Command**: `codex exec "prompt"`

**Default Model**: System default (user-configured)
- Uses your Codex CLI configuration
- Typically defaults to latest GPT model
- Override via flag: `codex exec --model gpt-5-codex "prompt"`

**Verification**:
```bash
which codex
codex exec "hello world"
```

**Features**:
- OpenAI backing (high quality)
- Open source on GitHub
- Read-only mode by default (safe for content processing)
- Supports multimodal inputs (text, screenshots, diagrams)

---

## Output Structure

Generated skills are organized as directory structures following [Skill_Seekers](../../../Skill_Seekers) patterns and Claude Skills best practices:

```
skills_output/
â””â”€â”€ credits-t4012-24e/              # Skill directory (named by skill ID)
    â”œâ”€â”€ SKILL.md                     # Lightweight index (< 500 lines)
    â”œâ”€â”€ references/                  # Detailed reference files
    â”‚   â”œâ”€â”€ index.md                # Navigation index
    â”‚   â”œâ”€â”€ carbon-rebate.md        # Chapter 1 (AI enhanced)
    â”‚   â”œâ”€â”€ tax-credits.md          # Chapter 2 (AI enhanced)
    â”‚   â””â”€â”€ deductions.md           # Chapter 3 (AI enhanced)
    â””â”€â”€ raw/                        # Original extracted content
        â””â”€â”€ full-extract.txt        # Complete 721K characters
```

### SKILL.md Format

```yaml
---
id: credits-t4012-24e
title: Tax Credits
tags: [Tax Credits, CRA, Canadian Tax, Corporation]
description: Comprehensive guide to tax credits
domain: tax
category: credits
priority: medium
quality_grade: B
source: ../mvp/pdf/t4012-24e.pdf
---

# Tax Credits

[Brief description]

## ðŸ“– When to Use This Skill
- Tax credits and rebates
- Credit eligibility and applications

## ðŸ“š Reference Documentation
1. [Carbon Rebate](references/carbon-rebate.md)
2. [Tax Credits](references/tax-credits.md)
3. [Deductions](references/deductions.md)

**[View Full Index](references/index.md)** | **[Raw Text](raw/full-extract.txt)**
```

### Reference File Format

Each reference file contains AI-enhanced content from one chapter/chunk:

```markdown
# Carbon Rebate for Small Businesses

**Chapter 1**

---

[Enhanced content with clear structure, examples, and actionable guidance]
```

---

## Performance Benchmarks

From test runs with real CRA PDFs (153 pages, 721K chars):

| Stage | Time | Notes |
|-------|------|-------|
| PDF Extraction | 1-5 sec | Full document; 153 pages ~0.5 sec |
| Classification | <1 sec | Keyword-based, very fast |
| Chapter Detection | <1 sec | Detects H1/H2 headings |
| Skill Metadata Generation | <1 sec | Template-based |
| **LLM Enhancement** | **Varies** | **Provider-optimized chunk sizes** |
| - Claude (300K chunks) | 5-8 min/chunk | **15-25 min for 3 chunks** |
| - Gemini (1.5M chunks) | 5-10 min/chunk | **5-10 min for 1 chunk** âš¡ |
| - Codex (250K chunks) | 5-7 min/chunk | **15-20 min for 3 chunks** |
| API Enhancement | 15-45 sec/chunk | **45-120 sec for 3 chunks** |
| Directory Structure Creation | <1 sec | SKILL.md + references + raw |

**Total Processing Time**:
- Without AI: ~10 seconds (full 153-page doc)
- With **Gemini CLI** (1 chunk): **5-10 minutes** âš¡ **FASTEST**
- With **Claude CLI** (3 chunks): **15-25 minutes** âœ… Processes ALL content
- With **Codex CLI** (3 chunks): **15-20 minutes**
- With API (3 chunks): **1-2 minutes** (requires API key)

**Provider-Optimized Chunk Sizes**:
- âœ… **Claude**: 300K chars (~75K tokens) - Optimized for 200K input limit
- âœ… **Gemini**: 1.5M chars (~375K tokens) - Leverages 1M token context window
- âœ… **Codex**: 250K chars (~62K tokens) - Conservative for stability

**Content Retention**:
- âœ… **100% of original text preserved** in `raw/full-extract.txt`
- âœ… **100% of content AI-enhanced** (split across reference files)
- âœ… **No truncation or data loss**

---

## Key Features & Improvements

### âœ… Provider-Optimized Chunk Sizes

**Status**: âœ… **IMPLEMENTED** (latest update)

Each LLM provider uses its optimal chunk size based on context window:

**Gemini** (1M token context):
- Chunk size: **1.5M chars** (~375K tokens)
- Performance: Processes 721K doc in **1 chunk** instead of 3
- Time savings: **~67% faster** (5-10 min vs 15-25 min)
- Best for: Large documents requiring speed

**Claude** (200K token context):
- Chunk size: **300K chars** (~75K tokens)
- Performance: Processes 721K doc in **3 chunks**
- Time: 15-25 minutes
- Best for: High quality with Claude Code subscription

**Codex** (128K token context):
- Chunk size: **250K chars** (~62K tokens)
- Performance: Processes 721K doc in **3 chunks**
- Time: 15-20 minutes
- Best for: OpenAI ecosystem users

**Impact**:
- ðŸš€ Gemini now **3x faster** than before
- âœ… No more "one size fits all" approach
- âœ… Each provider works at optimal capacity

### âœ… Complete Content Processing

**Status**: âœ… **IMPLEMENTED** (as of Phase-02 update)

- Intelligent chapter detection based on markdown headings
- Automatic chunking at chapter/paragraph boundaries
- Sequential processing of all chunks through chosen LLM
- Complete preservation of source content

**Before**: Only first 300K chars (42%) were enhanced, rest discarded
**Now**: All 721K chars (100%) are processed with provider-optimized chunks

### âœ… Progressive Disclosure Structure

**Status**: âœ… **IMPLEMENTED** following Skill_Seekers patterns

- SKILL.md as lightweight entry point (< 500 lines)
- references/ directory for detailed content (æŒ‰éœ€åŠ è½½)
- raw/ directory for complete source text
- Clear navigation with index files

**Benefits for CRA Q&A**:
- Claude only loads relevant chapters when needed
- Reduced context window usage
- Faster query response times

### âœ… No Data Loss

**Status**: âœ… **SOLVED**

- Original extracted text: `raw/full-extract.txt` (721K chars)
- Enhanced chunks: `references/*.md` (multiple files)
- Complete chapter coverage with metadata

---

## Remaining Considerations

### 1. Chapter Detection Quality

**Current Implementation**: Detects H1 (`#`) and H2 (`##`) markdown headings

**Limitation**: Some PDFs may not have clear heading structure

**Impact**: Falls back to paragraph-based chunking (still processes all content)

**Improvement**: Could add custom chapter pattern detection for specific CRA documents

### 2. Processing Time vs Completeness Trade-off

**Current**: 15-25 minutes to enhance full 721K document with local Claude

**Alternative Options**:
- Use API mode (1-2 minutes, requires API key and cost)
- Process specific page ranges with `--max-pages N`

**Recommendation**: For production, consider API mode or selective enhancement

### 3. Quality Scoring

**Current**: Validator checks YAML syntax and markdown format

**Not Measured**: Content accuracy, topic coverage, completeness

**Workaround**: Manual review of generated files

**Future**: Add semantic quality metrics

---

## Troubleshooting

### LLM CLI Not Found Errors

**`claude: command not found`**
- **Solution**: Install Claude Code and ensure CLI is in PATH
- Verify: `which claude`
- Install: Download from [claude.com](https://claude.com/claude-code)

**`gemini: command not found`**
- **Solution**: Install Gemini CLI via npm
- Install: `npm install -g @google/gemini-cli`
- Verify: `which gemini && gemini --version`

**`codex: command not found`**
- **Solution**: Install Codex CLI
- Install: `npm install -g @openai/codex` or `brew install codex`
- Verify: `which codex && codex --version`

### Enhancement Failures

**Local Claude doesn't enhance content**
- Check Claude Code subscription status (requires Code Max)
- Fallback: Use `--local-gemini` (free) or `--api` mode

**Gemini rate limit errors**
- Free tier limits: 60 requests/minute, 1000 requests/day
- Wait and retry, or use `--local-claude` or `--api` mode

**Codex authentication errors**
- Ensure OpenAI account has API access
- Check API key configuration
- Fallback: Use `--local-gemini` (free) or `--local-claude`

### PDF extraction fails

**Solutions**:
- Ensure PyMuPDF is installed: `uv sync`
- Check PDF is not corrupted or password-protected
- Try different PDF if issue persists

### Quality score seems too high

This is a known limitation. The validator checks format, not completeness.
- Review the actual content manually
- Check file size vs source PDF
- Count topics covered vs expected

---

## Next Steps

1. **Test with your PDF**:
   ```bash
   uv run python generate_skill.py --pdf your-file.pdf --local-claude --full
   ```

2. **Review generated skill files** in `skills_output/`

3. **Adjust for your needs**:
   - Modify classification categories in `content_classifier.py`
   - Customize skill templates in `skill_generator.py`
   - Tune enhancement prompts in `generate_skill.py`

4. **Integrate into production**:
   - Use Python API for programmatic access
   - Implement proper error handling
   - Add retry logic for Claude API calls
   - Consider batch processing for multiple PDFs

---

## Project Standards

All code follows `.claude/skills/development-policies`:
- English-only code and documentation
- Full type annotations with Pydantic models
- Structured logging with proper levels
- Comprehensive error handling
