# ä»»åŠ¡05ï¼šPDF æ–‡æœ¬æå–æ¨¡å—å¼€å‘ï¼ˆåŸºäº Skill Seekerï¼‰

## ä»»åŠ¡ç›®æ ‡

å¼€å‘ä¸€ä¸ª Python æ¨¡å—ï¼Œå°† PDF æ–‡æ¡£è½¬æ¢ä¸ºç»“æ„åŒ–æ–‡æœ¬å†…å®¹ï¼Œä¸ºæ„å»ºå¯æœç´¢çš„çŸ¥è¯†åº“åšå‡†å¤‡ã€‚æœ¬æ¨¡å—åŸºäº Skill Seeker é¡¹ç›®çš„ PyMuPDF å¤„ç†æ–¹æ¡ˆï¼Œä¸“é—¨é’ˆå¯¹ CRA ç¨åŠ¡æ–‡æ¡£ä¼˜åŒ–ã€‚

## æŠ€æœ¯è¦æ±‚

**æ ¸å¿ƒåº“ï¼š**
- `PyMuPDF (fitz)`ï¼šä¸»è¦çš„ PDF æ–‡æœ¬æå–åº“ï¼ˆåŸºäº Skill Seeker æ–¹æ¡ˆï¼‰
- `Pillow`ï¼šå›¾åƒå¤„ç†ï¼ˆå¯é€‰ï¼Œç”¨äºåµŒå…¥å›¾åƒï¼‰
- `pytesseract`ï¼šOCR åŠŸèƒ½ï¼ˆå¯é€‰ï¼Œç”¨äºæ‰«ææ–‡æ¡£ï¼‰

**æ€§èƒ½è¦æ±‚ï¼š**
- æ”¯æŒå¤§å‹ PDFï¼ˆ100+ é¡µï¼Œå¦‚ T4012 çš„ 152 é¡µï¼‰
- å†…å­˜å ç”¨ < 2GB
- å¹¶å‘å¤„ç†èƒ½åŠ›
- æ™ºèƒ½å†…å®¹åˆ†ç±»

**è¾“å‡ºè¦æ±‚ï¼š**
- ç»“æ„åŒ–æ–‡æœ¬å†…å®¹
- è¡¨æ ¼æ•°æ®æå–
- ç« èŠ‚ç»“æ„è¯†åˆ«
- å…ƒæ•°æ®æå–

## å®ç°æ­¥éª¤

### 1. åˆ›å»ºæ¨¡å—ç»“æ„

åœ¨é¡¹ç›®ä¸­åˆ›å»ºæ–‡æ¡£å¤„ç†æ¨¡å—ï¼š

```bash
mkdir -p backend/src/document_processor
mkdir -p backend/src/skills
touch backend/src/document_processor/__init__.py
touch backend/src/document_processor/pdf_extractor.py
touch backend/src/document_processor/content_classifier.py
touch backend/src/document_processor/skill_generator.py
```

### 2. å®ç°æ ¸å¿ƒæå–ç±»

è®¾è®¡ä¸€ä¸ª `PDFTextExtractor` ç±»ï¼Œæä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
- æ–‡æœ¬æå–ï¼ˆåŸºäº Skill Seeker çš„ pdf_extractor_poc.pyï¼‰
- è¡¨æ ¼æ£€æµ‹å’Œæ•°æ®æå–
- ç« èŠ‚ç»“æ„è¯†åˆ«
- å›¾åƒæå–ï¼ˆå¯é€‰ï¼‰
- OCR å¤„ç†ï¼ˆå¯é€‰ï¼‰

### 3. å†…å®¹è´¨é‡ä¼˜åŒ–

å®ç°æ™ºèƒ½å†…å®¹å¤„ç†ç­–ç•¥ï¼š
- ç« èŠ‚è¾¹ç•Œæ£€æµ‹
- è¡¨æ ¼æ•°æ®ç»“æ„åŒ–
- ä»£ç å—è¯†åˆ«
- é¡µçœ‰é¡µè„šè¿‡æ»¤
- é‡å¤å†…å®¹åˆå¹¶

### 4. é”™è¯¯å¤„ç†

å¤„ç†å¸¸è§é—®é¢˜ï¼š
- æŸåçš„ PDF æ–‡ä»¶
- å—å¯†ç ä¿æŠ¤çš„æ–‡ä»¶
- è¶…å¤§æ–‡ä»¶ï¼ˆ> 100MBï¼‰
- æ‰«ææ–‡æ¡£ OCR å¤„ç†
- ç¼–ç é—®é¢˜

### 5. CRA æ–‡æ¡£ä¸“ç”¨ä¼˜åŒ–

é’ˆå¯¹åŠ æ‹¿å¤§ç¨åŠ¡æ–‡æ¡£çš„ç‰¹æ®Šå¤„ç†ï¼š
- ç¨åŠ¡æœ¯è¯­è¯†åˆ«
- è¡¨æ ¼åˆ†ç±»ï¼ˆç¨åŠ¡è¡¨æ ¼ã€è®¡ç®—è¡¨æ ¼ï¼‰
- æ³•è§„æ¡æ¬¾ç»“æ„åŒ–
- äº¤å‰å¼•ç”¨é“¾æ¥

## å…³é”®ä»£ç æç¤º

**æ ¸å¿ƒæå–å™¨å®ç°ï¼ˆåŸºäº Skill Seekerï¼‰ï¼š**

```python
import fitz  # PyMuPDF
import json
import re
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass

@dataclass
class ChapterInfo:
    """ç« èŠ‚ä¿¡æ¯"""
    title: str
    start_page: int
    end_page: int
    level: int

@dataclass
class TableData:
    """è¡¨æ ¼æ•°æ®"""
    title: str
    rows: List[List[str]]
    page: int
    bbox: Tuple[float, float, float, float]

class PDFTextExtractor:
    """PDF æ–‡æœ¬æå–å™¨ï¼ˆåŸºäº Skill Seeker æ–¹æ¡ˆï¼‰"""

    def __init__(
        self,
        pdf_path: str,
        verbose: bool = True,
        extract_images: bool = False,
        extract_tables: bool = True,
        ocr_enabled: bool = False
    ):
        self.pdf_path = pdf_path
        self.verbose = verbose
        self.extract_images = extract_images
        self.extract_tables = extract_tables
        self.ocr_enabled = ocr_enabled

        # CRA æ–‡æ¡£ä¸“ç”¨é…ç½®
        self.tax_keywords = [
            "capital gains", "business income", "tax credits",
            "deductions", "RRSP", "GST/HST", "T4012"
        ]

        # åˆå§‹åŒ– PyMuPDF
        self.doc = None
        self.total_pages = 0

    def open_document(self) -> bool:
        """æ‰“å¼€ PDF æ–‡æ¡£"""
        try:
            self.doc = fitz.open(self.pdf_path)
            self.total_pages = len(self.doc)
            if self.verbose:
                print(f"âœ… æ‰“å¼€ PDF: {self.pdf_path}")
                print(f"ğŸ“„ æ€»é¡µæ•°: {self.total_pages}")
            return True
        except Exception as e:
            print(f"âŒ æ— æ³•æ‰“å¼€ PDF: {e}")
            return False

    def extract_all(self) -> Dict:
        """æå–æ‰€æœ‰å†…å®¹"""
        if not self.doc:
            self.open_document()

        result = {
            "metadata": self._extract_metadata(),
            "chapters": self._detect_chapters(),
            "pages": [],
            "tables": [],
            "images": []
        }

        # æŒ‰ç« èŠ‚æå–å†…å®¹
        for chapter in result["chapters"]:
            chapter_content = self._extract_chapter_content(chapter)
            result["pages"].extend(chapter_content)

        # æå–è¡¨æ ¼
        if self.extract_tables:
            result["tables"] = self._extract_tables()

        # æå–å›¾åƒ
        if self.extract_images:
            result["images"] = self._extract_images()

        return result

    def _extract_metadata(self) -> Dict:
        """æå– PDF å…ƒæ•°æ®"""
        if not self.doc:
            return {}

        metadata = self.doc.metadata
        return {
            "title": metadata.get("title", ""),
            "author": metadata.get("author", ""),
            "subject": metadata.get("subject", ""),
            "creator": metadata.get("creator", ""),
            "producer": metadata.get("producer", ""),
            "creation_date": metadata.get("creationDate", ""),
            "modification_date": metadata.get("modDate", ""),
            "page_count": self.total_pages,
            "is_encrypted": self.doc.is_encrypted
        }

    def _detect_chapters(self) -> List[ChapterInfo]:
        """æ£€æµ‹ç« èŠ‚ç»“æ„ï¼ˆCRA æ–‡æ¡£ä¸“ç”¨ï¼‰"""
        chapters = []
        current_chapter = None

        # CRA æ–‡æ¡£ç« èŠ‚æ¨¡å¼
        chapter_patterns = [
            r"^Chapter\s+(\d+)\s*[:\-]\s*(.+)$",
            r"^ç¬¬(\d+)ç« \s*[:\-]\s*(.+)$",
            r"^(\d+)\.\s*(.+)$",  # æ•°å­—ç¼–å·
            r"^(.+)\s*\n\s*=+$",  # æ ‡é¢˜ + ä¸‹åˆ’çº¿
        ]

        for page_num in range(self.total_pages):
            page = self.doc[page_num]
            text = page.get_text()

            for pattern in chapter_patterns:
                matches = re.finditer(pattern, text, re.MULTILINE)
                for match in matches:
                    if current_chapter:
                        current_chapter.end_page = page_num
                        chapters.append(current_chapter)

                    chapter_num = match.group(1) if match.groups() else str(len(chapters) + 1)
                    chapter_title = match.group(2) if len(match.groups()) > 1 else match.group(1)

                    current_chapter = ChapterInfo(
                        title=chapter_title.strip(),
                        start_page=page_num,
                        end_page=self.total_pages - 1,
                        level=0
                    )

                    if self.verbose:
                        print(f"ğŸ“– å‘ç°ç« èŠ‚: {chapter_title} (é¡µ {page_num})")

        # å¤„ç†æœ€åä¸€ç« 
        if current_chapter and current_chapter not in chapters:
            chapters.append(current_chapter)

        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ç« èŠ‚ï¼Œåˆ›å»ºé»˜è®¤ç« èŠ‚
        if not chapters:
            chapters.append(ChapterInfo(
                title="å®Œæ•´æ–‡æ¡£",
                start_page=0,
                end_page=self.total_pages - 1,
                level=0
            ))

        return chapters

    def _extract_chapter_content(self, chapter: ChapterInfo) -> List[Dict]:
        """æå–ç« èŠ‚å†…å®¹"""
        content_pages = []

        for page_num in range(chapter.start_page, chapter.end_page + 1):
            page = self.doc[page_num]

            # æå–æ–‡æœ¬
            text = page.get_text()

            # æ¸…ç†æ–‡æœ¬
            cleaned_text = self._clean_text(text)

            # æå–å…³é”®ä¿¡æ¯
            page_data = {
                "page_number": page_num + 1,
                "chapter": chapter.title,
                "text": cleaned_text,
                "keywords": self._extract_keywords(cleaned_text),
                "sections": self._detect_sections(cleaned_text),
                "has_tables": self._page_has_tables(page),
                "word_count": len(cleaned_text.split())
            }

            content_pages.append(page_data)

        return content_pages

    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æå–çš„æ–‡æœ¬"""
        # ç§»é™¤é¡µçœ‰é¡µè„šï¼ˆCRA æ–‡æ¡£å¸¸è§æ¨¡å¼ï¼‰
        text = re.sub(r'^.*?Canada Revenue Agency.*?\n', '', text, flags=re.MULTILINE)
        text = re.sub(r'^.*?Agence du revenu du Canada.*?\n', '', text, flags=re.MULTILINE)

        # ç§»é™¤é¡µç 
        text = re.sub(r'^\s*\d+\s*$', '', text, flags=re.MULTILINE)

        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)

        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s\n\t\.\,\;\:\!\?\-\(\)\/\$\%\[\]]+', ' ', text)

        return text.strip()

    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆç¨åŠ¡ä¸“ç”¨ï¼‰"""
        keywords = []
        text_lower = text.lower()

        for keyword in self.tax_keywords:
            if keyword in text_lower:
                keywords.append(keyword)

        # æå–å¤§å†™æœ¯è¯­ï¼ˆå¯èƒ½æ˜¯æ³•å¾‹æœ¯è¯­ï¼‰
        uppercase_terms = re.findall(r'\b[A-Z]{2,}\b', text)
        keywords.extend([term.lower() for term in uppercase_terms if len(term) > 3])

        return list(set(keywords))

    def _detect_sections(self, text: str) -> List[Dict]:
        """æ£€æµ‹æ®µè½ç»“æ„"""
        sections = []

        # æ£€æµ‹æ ‡é¢˜æ¨¡å¼
        heading_patterns = [
            r'^([A-Z][^.!?]*)\s*$',  # å…¨å¤§å†™æ ‡é¢˜
            r'^(\d+\.\d+)\s+(.+)$',   # æ•°å­—ç¼–å·
            r'^([A-Z][a-z]+[^.!?]*)\s*$',  # é¦–å­—æ¯å¤§å†™æ ‡é¢˜
        ]

        lines = text.split('\n')
        current_section = None

        for line in lines:
            line = line.strip()
            if not line:
                continue

            for pattern in heading_patterns:
                if re.match(pattern, line):
                    if current_section:
                        sections.append(current_section)

                    current_section = {
                        "title": line,
                        "content": "",
                        "level": 0
                    }
                    break
            else:
                if current_section:
                    current_section["content"] += line + " "

        if current_section:
            sections.append(current_section)

        return sections

    def _extract_tables(self) -> List[TableData]:
        """æå–è¡¨æ ¼æ•°æ®"""
        tables = []

        for page_num in range(self.total_pages):
            page = self.doc[page_num]

            # ä½¿ç”¨ PyMuPDF çš„è¡¨æ ¼æ£€æµ‹
            table_list = page.find_tables()

            for table in table_list:
                try:
                    # æå–è¡¨æ ¼æ•°æ®
                    table_data = table.extract()

                    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                    rows = []
                    for row in table_data:
                        rows.append([str(cell) for cell in row])

                    table_info = TableData(
                        title=f"è¡¨æ ¼_é¡µ{page_num + 1}_{len(tables) + 1}",
                        rows=rows,
                        page=page_num + 1,
                        bbox=table.bbox
                    )

                    tables.append(table_info)

                    if self.verbose:
                        print(f"ğŸ“Š æå–è¡¨æ ¼: {table_info.title} ({len(rows)} è¡Œ)")

                except Exception as e:
                    if self.verbose:
                        print(f"âš ï¸ è¡¨æ ¼æå–å¤±è´¥ (é¡µ {page_num + 1}): {e}")

        return tables

    def _page_has_tables(self, page) -> bool:
        """æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«è¡¨æ ¼"""
        try:
            table_list = page.find_tables()
            return len(table_list) > 0
        except:
            return False

    def _extract_images(self) -> List[Dict]:
        """æå–å›¾åƒä¿¡æ¯"""
        images = []

        for page_num in range(self.total_pages):
            page = self.doc[page_num]
            image_list = page.get_images()

            for img_index, img in enumerate(image_list):
                try:
                    xref = img[0]
                    base_image = self.doc.extract_image(xref)

                    image_info = {
                        "page": page_num + 1,
                        "index": img_index,
                        "xref": xref,
                        "width": base_image.get("width", 0),
                        "height": base_image.get("height", 0),
                        "colorspace": base_image.get("colorspace", ""),
                        "size": len(base_image.get("image", b""))
                    }

                    images.append(image_info)

                    if self.verbose:
                        print(f"ğŸ–¼ï¸ å‘ç°å›¾åƒ: é¡µ {page_num + 1}, å°ºå¯¸ {image_info['width']}x{image_info['height']}")

                except Exception as e:
                    if self.verbose:
                        print(f"âš ï¸ å›¾åƒæå–å¤±è´¥ (é¡µ {page_num + 1}): {e}")

        return images

    def close_document(self):
        """å…³é—­æ–‡æ¡£"""
        if self.doc:
            self.doc.close()
            self.doc = None

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close_document()


# ä½¿ç”¨ç¤ºä¾‹
def extract_cra_document(pdf_path: str, output_dir: str = "output") -> Dict:
    """æå– CRA æ–‡æ¡£çš„å®Œæ•´æµç¨‹"""

    extractor = PDFTextExtractor(
        pdf_path=pdf_path,
        verbose=True,
        extract_tables=True,
        extract_images=False
    )

    try:
        # æå–å†…å®¹
        result = extractor.extract_all()

        # ä¿å­˜ç»“æœ
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # ä¿å­˜ JSON æ ¼å¼
        json_path = output_path / f"{Path(pdf_path).stem}_extracted.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

        print(f"âœ… æå–å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {json_path}")

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æå–ç»Ÿè®¡:")
        print(f"  - ç« èŠ‚: {len(result['chapters'])}")
        print(f"  - é¡µé¢: {len(result['pages'])}")
        print(f"  - è¡¨æ ¼: {len(result['tables'])}")
        print(f"  - æ€»å­—æ•°: {sum(p['word_count'] for p in result['pages'])}")

        return result

    finally:
        extractor.close_document()


if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    pdf_path = "t4012-24e.pdf"
    result = extract_cra_document(pdf_path)
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åŸºç¡€ç”¨æ³•
extractor = PDFTextExtractor("t4012-24e.pdf", extract_tables=True)
result = extractor.extract_all()

# åªæå–ç‰¹å®šé¡µé¢èŒƒå›´
extractor = PDFTextExtractor("document.pdf")
chapters = extractor._detect_chapters()
for chapter in chapters:
    if "Capital Gains" in chapter.title:
        content = extractor._extract_chapter_content(chapter)

# è·å–æ–‡æ¡£ä¿¡æ¯
metadata = extractor._extract_metadata()
print(f"æ–‡æ¡£æ ‡é¢˜: {metadata['title']}")
print(f"é¡µæ•°: {metadata['page_count']}")
```

## æµ‹è¯•éªŒè¯

### 1. å•å…ƒæµ‹è¯•

åˆ›å»º `tests/test_pdf_extractor.py`ï¼š

```python
import pytest
import tempfile
import json
from backend.src.document_processor.pdf_extractor import PDFTextExtractor

def test_extract_simple_pdf(tmp_path):
    # å‡è®¾æœ‰æµ‹è¯• PDF
    extractor = PDFTextExtractor("tests/fixtures/t4012_sample.pdf")
    result = extractor.extract_all()

    assert len(result['pages']) > 0
    assert len(result['chapters']) > 0
    assert 'metadata' in result
    extractor.close_document()

def test_chapter_detection(tmp_path):
    extractor = PDFTextExtractor("tests/fixtures/t4012_sample.pdf")
    chapters = extractor._detect_chapters()

    assert len(chapters) > 0
    assert all(isinstance(chapter.start_page, int) for chapter in chapters)
    assert all(isinstance(chapter.end_page, int) for chapter in chapters)
    extractor.close_document()

def test_table_extraction(tmp_path):
    extractor = PDFTextExtractor("tests/fixtures/t4012_sample.pdf", extract_tables=True)
    tables = extractor._extract_tables()

    # T4012 åº”è¯¥åŒ…å«è¡¨æ ¼
    assert len(tables) > 0
    assert all(len(table.rows) > 0 for table in tables)
    extractor.close_document()

def test_cra_keywords(tmp_path):
    extractor = PDFTextExtractor("tests/fixtures/t4012_sample.pdf")

    # æµ‹è¯•ç¨åŠ¡å…³é”®è¯æå–
    test_text = "This document discusses capital gains and RRSP contributions."
    keywords = extractor._extract_keywords(test_text)

    assert "capital gains" in keywords
    assert "rrsp" in keywords
    extractor.close_document()
```

è¿è¡Œæµ‹è¯•ï¼š
```bash
pytest tests/test_pdf_extractor.py -v
```

### 2. æ€§èƒ½æµ‹è¯•

æµ‹è¯•å¤§æ–‡ä»¶å¤„ç†ï¼š
```python
import time
import psutil
import os

def test_large_pdf_performance():
    process = psutil.Process(os.getpid())

    extractor = PDFTextExtractor("t4012-24e.pdf")

    # ç›‘æ§å†…å­˜
    mem_before = process.memory_info().rss / 1024 / 1024  # MB

    start_time = time.time()
    result = extractor.extract_all()
    duration = time.time() - start_time

    mem_after = process.memory_info().rss / 1024 / 1024  # MB
    mem_peak = max(mem_before, mem_after)

    print(f"å¤„ç† {result['metadata']['page_count']} é¡µ PDF è€—æ—¶: {duration:.2f} ç§’")
    print(f"å¹³å‡æ¯é¡µ: {duration/result['metadata']['page_count']:.2f} ç§’")
    print(f"å†…å­˜å³°å€¼: {mem_peak:.2f} MB")

    # æ€§èƒ½è¦æ±‚æ£€æŸ¥
    assert duration < 300  # 5åˆ†é’Ÿå†…å®Œæˆ
    assert mem_peak < 2048  # å†…å­˜å°äº 2GB

    extractor.close_document()
```

### 3. CRA æ–‡æ¡£ä¸“ç”¨æµ‹è¯•

```python
def test_t4012_specific_features():
    extractor = PDFTextExtractor("t4012-24e.pdf")
    result = extractor.extract_all()

    # æ£€æŸ¥æ˜¯å¦åŒ…å« T4012 ç‰¹å®šå†…å®¹
    all_text = " ".join(page['text'] for page in result['pages'])

    # T4012 åº”è¯¥åŒ…å«çš„å…³é”®å†…å®¹
    required_terms = [
        "T4012",
        "Capital Gains",
        "Business Income",
        "Tax Guide",
        "CRA"
    ]

    for term in required_terms:
        assert term.lower() in all_text.lower(), f"ç¼ºå°‘å¿…è¦æœ¯è¯­: {term}"

    # æ£€æŸ¥ç« èŠ‚ç»“æ„
    assert len(result['chapters']) >= 3, "T4012 åº”è¯¥è‡³å°‘æœ‰ 3 ä¸ªç« èŠ‚"

    # æ£€æŸ¥è¡¨æ ¼
    if result['tables']:
        assert len(result['tables']) > 0, "T4012 åº”è¯¥åŒ…å«è¡¨æ ¼"

    extractor.close_document()
```

## æ³¨æ„äº‹é¡¹

**CRA æ–‡æ¡£ç‰¹ç‚¹ï¼š**
- **åŒè¯­å†…å®¹**ï¼šè‹±è¯­å’Œæ³•è¯­æ··åˆ
- **å¤æ‚è¡¨æ ¼**ï¼šç¨åŠ¡è®¡ç®—è¡¨æ ¼
- **æ³•å¾‹æ¡æ¬¾**ï¼šç²¾ç¡®çš„æ³•å¾‹æ–‡æœ¬
- **äº¤å‰å¼•ç”¨**ï¼šå¤§é‡å†…éƒ¨é“¾æ¥

**å¤„ç†ç­–ç•¥ï¼š**
1. **è¯­è¨€æ£€æµ‹**ï¼šåŒºåˆ†è‹±è¯­/æ³•è¯­å†…å®¹
2. **è¡¨æ ¼ä¿ç•™**ï¼šä¿æŒè¡¨æ ¼æ•°æ®çš„å®Œæ•´æ€§
3. **æ³•å¾‹æœ¯è¯­**ï¼šç²¾ç¡®æå–ï¼Œä¸åšç®€åŒ–
4. **å¼•ç”¨å¤„ç†**ï¼šç»´æŠ¤ç« èŠ‚é—´çš„å¼•ç”¨å…³ç³»

**å†…å­˜ä¼˜åŒ–ï¼š**
1. åˆ†é¡µå¤„ç†é¿å…ä¸€æ¬¡æ€§åŠ è½½
2. åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„é¡µé¢æ•°æ®
3. å¤§å‹è¡¨æ ¼å•ç‹¬å¤„ç†
4. å›¾åƒæå–å¯é€‰ï¼ˆèŠ‚çœå†…å­˜ï¼‰

**é”™è¯¯æ¢å¤ï¼š**
```python
try:
    extractor = PDFTextExtractor("document.pdf")
    result = extractor.extract_all()
except Exception as e:
    # å›é€€ç­–ç•¥ï¼šä½¿ç”¨å¤‡ç”¨æå–æ–¹æ³•
    result = fallback_extraction("document.pdf")
```

## ä¸ Skill Seeker çš„é›†æˆ

**ä»£ç å¤ç”¨ï¼š**
- åŸºäº Skill Seeker çš„ `pdf_extractor_poc.py` æ ¸å¿ƒé€»è¾‘
- ä¿ç•™è¡¨æ ¼æ£€æµ‹å’Œç« èŠ‚è¯†åˆ«åŠŸèƒ½
- é€‚é… CRA æ–‡æ¡£çš„ç‰¹æ®Šéœ€æ±‚

**å¢å¼ºåŠŸèƒ½ï¼š**
- CRA ä¸“ç”¨å…³é”®è¯åº“
- ç¨åŠ¡æœ¯è¯­è¯†åˆ«
- æ³•è§„æ¡æ¬¾ç»“æ„åŒ–
- åŒè¯­å†…å®¹å¤„ç†

**è¾“å‡ºæ ¼å¼ï¼š**
- å…¼å®¹ Skill Seeker çš„ JSON æ ¼å¼
- å¢åŠ ç¨åŠ¡ä¸“ç”¨å­—æ®µ
- ä¿æŒå‘åå…¼å®¹æ€§

## ä¾èµ–å…³ç³»

**æ–°å¢ä¾èµ–ï¼š**
```toml
# å·²åœ¨ pyproject.toml ä¸­ç¡®è®¤
PyMuPDF>=1.24.0          # æ ¸å¿ƒ PDF å¤„ç†
Pillow>=10.0.0           # å›¾åƒå¤„ç†
pytesseract>=0.3.13      # OCRï¼ˆå¯é€‰ï¼‰
```

**å‰ç½®ä»»åŠ¡ï¼š**
- ä»»åŠ¡04ï¼šPython ä¾èµ–ç¯å¢ƒå®‰è£…

**åç½®ä»»åŠ¡ï¼š**
- ä»»åŠ¡06ï¼šå†…å®¹åˆ†ç±»æ¨¡å—ï¼ˆä½¿ç”¨æå–çš„å†…å®¹ï¼‰
- ä»»åŠ¡07ï¼šSkill ç”Ÿæˆæ¨¡å—ï¼ˆè½¬æ¢ä¸º Skill æ ¼å¼ï¼‰

è¿™ä¸ªæ¨¡å—ä¸º CRA æ–‡æ¡£å¤„ç†æä¾›äº†å®Œæ•´çš„è§£å†³æ–¹æ¡ˆï¼ŒåŸºäº Skill Seeker çš„æˆç†ŸæŠ€æœ¯æ ˆï¼Œä¸“é—¨é’ˆå¯¹ç¨åŠ¡æ–‡æ¡£çš„ç‰¹ç‚¹è¿›è¡Œäº†ä¼˜åŒ–ã€‚