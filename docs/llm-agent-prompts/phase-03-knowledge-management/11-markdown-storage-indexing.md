# ä»»åŠ¡11ï¼šè½»é‡çº§ Skill ç´¢å¼•ç³»ç»Ÿ

## ä»»åŠ¡ç›®æ ‡

å®ç°è½»é‡çº§çš„ Skill ç´¢å¼•å’ŒåŠ è½½ç³»ç»Ÿï¼Œæ”¯æŒå¿«é€ŸæŸ¥æ‰¾ã€åŠ è½½å’Œç®¡ç† Skillsã€‚ç³»ç»Ÿé‡‡ç”¨ç®€å•çš„æ–‡ä»¶ç³»ç»Ÿ + JSON ç´¢å¼•æ–¹æ¡ˆï¼Œæ— éœ€å‘é‡æ•°æ®åº“æˆ–å…¨æ–‡æ£€ç´¢å¼•æ“ï¼Œå……åˆ†åˆ©ç”¨ Claude 200K context çª—å£çš„ä¼˜åŠ¿ã€‚

## æŠ€æœ¯è¦æ±‚

**å­˜å‚¨æ–¹æ¡ˆï¼š**
- æ–‡ä»¶ç³»ç»Ÿï¼šMarkdown æ–‡ä»¶å­˜å‚¨ï¼ˆå¸¦ YAML front matterï¼‰
- JSON ç´¢å¼•ï¼šè½»é‡çº§å…ƒæ•°æ®ç´¢å¼•
- æ— éœ€å‘é‡æ•°æ®åº“ï¼ˆChromaDBã€Qdrant ç­‰ï¼‰
- æ— éœ€å…¨æ–‡æ£€ç´¢å¼•æ“ï¼ˆSQLite FTS5ã€Elasticsearch ç­‰ï¼‰

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
- YAML front matter è§£æ
- è‡ªåŠ¨ç´¢å¼•æ„å»ºå’Œæ›´æ–°
- Skill å¿«é€ŸæŸ¥æ‰¾å’ŒåŠ è½½
- å…ƒæ•°æ®æŸ¥è¯¢å’Œè¿‡æ»¤

**æ€§èƒ½è¦æ±‚ï¼š**
- ç´¢å¼•æ„å»ºæ—¶é—´ < 1ç§’ï¼ˆ100ä¸ª Skillsï¼‰
- Skill åŠ è½½æ—¶é—´ < 50ms
- å†…å­˜å ç”¨ < 10MBï¼ˆç´¢å¼•ï¼‰
- æ”¯æŒçƒ­æ›´æ–°ï¼ˆæ–‡ä»¶å˜åŒ–è‡ªåŠ¨é‡å»ºç´¢å¼•ï¼‰

## å®ç°æ­¥éª¤

### 1. è®¾è®¡ç´¢å¼•ç»“æ„

å®šä¹‰è½»é‡çº§ JSON ç´¢å¼•æ ¼å¼ï¼š
```json
{
  "skill_id_1": {
    "path": "federal/personal-income-tax.md",
    "metadata": {
      "skill_id": "skill_id_1",
      "title": "...",
      "domain": "...",
      "triggers": [...],
      "keywords": [...],
      "related_skills": [...],
      "content_size_kb": 25.5,
      "estimated_tokens": 6500
    }
  }
}
```

### 2. å®ç° YAML Parser

å¼€å‘ YAML front matter è§£æå™¨ï¼š
- è¯»å– Markdown æ–‡ä»¶
- æå– `---` åˆ†éš”çš„ YAML éƒ¨åˆ†
- è§£æä¸º Python å­—å…¸
- éªŒè¯å¿…éœ€å­—æ®µ

### 3. å®ç°ç´¢å¼•æ„å»ºå™¨

å¼€å‘è‡ªåŠ¨ç´¢å¼•æ„å»ºç³»ç»Ÿï¼š
- æ‰«æ skills ç›®å½•
- è§£ææ‰€æœ‰ .md æ–‡ä»¶
- æ„å»ºå†…å­˜ç´¢å¼•
- ä¿å­˜åˆ° JSON æ–‡ä»¶

### 4. å®ç° Skill åŠ è½½å™¨

å¼€å‘ Skill å†…å®¹åŠ è½½ç³»ç»Ÿï¼š
- æ ¹æ® skill_id å¿«é€Ÿå®šä½æ–‡ä»¶
- è¯»å–æ–‡ä»¶å†…å®¹
- ç§»é™¤ front matter è¿”å›çº¯å†…å®¹
- æ”¯æŒæ‰¹é‡åŠ è½½

## å…³é”®ä»£ç æç¤º

**Skill Loader æ ¸å¿ƒå®ç°ï¼š**

```python
import yaml
import json
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
import hashlib
import time

@dataclass
class SkillMetadata:
    """Skill å…ƒæ•°æ®"""
    skill_id: str
    title: str
    domain: str
    path: str  # ç›¸å¯¹è·¯å¾„
    topics: List[str]
    triggers: List[str]
    keywords: List[str]
    related_skills: List[Dict]
    content_size_kb: float
    estimated_tokens: int
    version: str = "1.0"
    last_updated: str = ""

    def to_dict(self) -> Dict:
        return asdict(self)


class SkillIndexBuilder:
    """Skill ç´¢å¼•æ„å»ºå™¨"""

    def __init__(self, skills_dir: str = "knowledge_base/skills"):
        self.skills_dir = Path(skills_dir)
        self.index: Dict[str, SkillMetadata] = {}

    def build(self) -> Dict[str, Dict]:
        """
        æ‰«æå¹¶æ„å»ºç´¢å¼•

        Returns:
            {skill_id: {path, metadata}}
        """
        start_time = time.time()
        self.index = {}

        # æ‰«ææ‰€æœ‰ .md æ–‡ä»¶
        md_files = list(self.skills_dir.rglob("*.md"))

        for md_file in md_files:
            # è·³è¿‡ç´¢å¼•æ–‡ä»¶
            if md_file.name == "skills-index.json":
                continue

            try:
                metadata = self._parse_skill_file(md_file)
                if metadata:
                    self.index[metadata.skill_id] = {
                        "path": str(md_file.relative_to(self.skills_dir)),
                        "metadata": metadata.to_dict()
                    }
            except Exception as e:
                print(f"âš ï¸  è§£æ {md_file.name} å¤±è´¥: {e}")

        duration = time.time() - start_time
        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼š{len(self.index)} ä¸ª Skillsï¼Œè€—æ—¶ {duration:.2f}s")

        return self.index

    def _parse_skill_file(self, md_file: Path) -> Optional[SkillMetadata]:
        """è§£æå•ä¸ª Skill æ–‡ä»¶"""
        with open(md_file, "r", encoding="utf-8") as f:
            content = f.read()

        # æ£€æŸ¥ front matter
        if not content.startswith("---"):
            print(f"âš ï¸  {md_file.name} ç¼ºå°‘ YAML front matter")
            return None

        # åˆ†å‰²å†…å®¹
        parts = content.split("---", 2)
        if len(parts) < 3:
            return None

        yaml_content = parts[1]
        markdown_content = parts[2].strip()

        # è§£æ YAML
        yaml_data = yaml.safe_load(yaml_content)

        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ["skill_id", "title", "domain"]
        for field in required_fields:
            if field not in yaml_data:
                print(f"âš ï¸  {md_file.name} ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                return None

        # è®¡ç®—æ–‡ä»¶å¤§å°
        file_size_kb = md_file.stat().st_size / 1024

        # ä¼°ç®— token æ•°ï¼ˆç²—ç•¥ï¼šä¸­æ–‡ 2å­—ç¬¦/tokenï¼Œè‹±æ–‡ 4å­—ç¬¦/tokenï¼‰
        estimated_tokens = int(len(markdown_content) * 0.5)

        # æ„å»ºå…ƒæ•°æ®
        return SkillMetadata(
            skill_id=yaml_data["skill_id"],
            title=yaml_data["title"],
            domain=yaml_data["domain"],
            path=str(md_file.relative_to(self.skills_dir)),
            topics=yaml_data.get("topics", []),
            triggers=yaml_data.get("triggers", []),
            keywords=yaml_data.get("keywords", []),
            related_skills=yaml_data.get("related_skills", []),
            content_size_kb=round(file_size_kb, 2),
            estimated_tokens=estimated_tokens,
            version=yaml_data.get("version", "1.0"),
            last_updated=yaml_data.get("last_updated", "")
        )

    def save(self, output_path: Optional[Path] = None):
        """ä¿å­˜ç´¢å¼•åˆ° JSON æ–‡ä»¶"""
        if output_path is None:
            output_path = self.skills_dir / "skills-index.json"

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(self.index, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ ç´¢å¼•å·²ä¿å­˜: {output_path}")


class SkillLoader:
    """Skill åŠ è½½å™¨"""

    def __init__(self, skills_dir: str = "knowledge_base/skills"):
        self.skills_dir = Path(skills_dir)
        self.index: Dict[str, Dict] = {}
        self._load_index()

    def _load_index(self):
        """åŠ è½½ç´¢å¼•æ–‡ä»¶"""
        index_file = self.skills_dir / "skills-index.json"

        if not index_file.exists():
            print("âš ï¸  ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£åœ¨æ„å»º...")
            self.rebuild_index()
            return

        with open(index_file, "r", encoding="utf-8") as f:
            self.index = json.load(f)

        print(f"ğŸ“š å·²åŠ è½½ {len(self.index)} ä¸ª Skills ç´¢å¼•")

    def rebuild_index(self):
        """é‡å»ºç´¢å¼•"""
        builder = SkillIndexBuilder(str(self.skills_dir))
        self.index = builder.build()
        builder.save()

    def get_skill(self, skill_id: str) -> Optional[Dict]:
        """
        è·å–æŒ‡å®š Skill çš„å®Œæ•´å†…å®¹

        Returns:
            {
                "skill_id": "...",
                "metadata": {...},
                "content": "...",  # çº¯ Markdown å†…å®¹ï¼ˆæ—  front matterï¼‰
                "path": "..."
            }
        """
        if skill_id not in self.index:
            return None

        skill_info = self.index[skill_id]
        skill_path = self.skills_dir / skill_info["path"]

        # è¯»å–æ–‡ä»¶
        with open(skill_path, "r", encoding="utf-8") as f:
            full_content = f.read()

        # ç§»é™¤ front matter
        if full_content.startswith("---"):
            parts = full_content.split("---", 2)
            content = parts[2].strip() if len(parts) >= 3 else full_content
        else:
            content = full_content

        return {
            "skill_id": skill_id,
            "metadata": skill_info["metadata"],
            "content": content,
            "path": str(skill_path)
        }

    def load_skills(self, skill_ids: List[str]) -> List[Dict]:
        """æ‰¹é‡åŠ è½½ Skills"""
        loaded = []

        for skill_id in skill_ids:
            skill = self.get_skill(skill_id)
            if skill:
                loaded.append(skill)
            else:
                print(f"âš ï¸  Skill '{skill_id}' ä¸å­˜åœ¨")

        return loaded

    def search(
        self,
        domain: Optional[str] = None,
        triggers: Optional[List[str]] = None,
        keywords: Optional[List[str]] = None
    ) -> List[str]:
        """
        æœç´¢ Skillsï¼ˆç®€å•çš„å…ƒæ•°æ®åŒ¹é…ï¼‰

        Returns:
            List of skill_ids
        """
        results = []

        for skill_id, skill_info in self.index.items():
            metadata = skill_info["metadata"]

            # é¢†åŸŸè¿‡æ»¤
            if domain and metadata.get("domain") != domain:
                continue

            # è§¦å‘è¯åŒ¹é…
            if triggers:
                skill_triggers = metadata.get("triggers", [])
                if not any(t.lower() in [st.lower() for st in skill_triggers] for t in triggers):
                    continue

            # å…³é”®è¯åŒ¹é…
            if keywords:
                skill_keywords = metadata.get("keywords", [])
                if not any(kw.lower() in " ".join(skill_keywords).lower() for kw in keywords):
                    continue

            results.append(skill_id)

        return results

    def get_related_skills(
        self,
        skill_id: str,
        priority: Optional[str] = None
    ) -> List[str]:
        """
        è·å–ç›¸å…³ Skills

        Args:
            skill_id: ä¸» Skill ID
            priority: ä¼˜å…ˆçº§è¿‡æ»¤ï¼ˆhigh/medium/lowï¼‰ï¼ŒNone è¿”å›å…¨éƒ¨

        Returns:
            List of related skill_ids
        """
        if skill_id not in self.index:
            return []

        metadata = self.index[skill_id]["metadata"]
        related = metadata.get("related_skills", [])

        if priority:
            related = [r for r in related if r.get("priority") == priority]

        return [r["id"] for r in related]

    def get_all_skill_ids(self) -> List[str]:
        """è·å–æ‰€æœ‰ Skill IDs"""
        return list(self.index.keys())

    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        domains = {}
        total_size = 0
        total_tokens = 0

        for skill_info in self.index.values():
            metadata = skill_info["metadata"]
            domain = metadata.get("domain", "unknown")

            domains[domain] = domains.get(domain, 0) + 1
            total_size += metadata.get("content_size_kb", 0)
            total_tokens += metadata.get("estimated_tokens", 0)

        return {
            "total_skills": len(self.index),
            "domains": domains,
            "total_size_kb": round(total_size, 2),
            "total_estimated_tokens": total_tokens,
            "avg_skill_size_kb": round(total_size / len(self.index), 2) if self.index else 0
        }


class SkillCache:
    """Skill å†…å®¹ç¼“å­˜ï¼ˆå¯é€‰ä¼˜åŒ–ï¼‰"""

    def __init__(self, max_size: int = 20):
        self.max_size = max_size
        self.cache: Dict[str, Dict] = {}
        self.access_order: List[str] = []

    def get(self, skill_id: str) -> Optional[Dict]:
        """ä»ç¼“å­˜è·å–"""
        if skill_id in self.cache:
            # æ›´æ–°è®¿é—®é¡ºåºï¼ˆLRUï¼‰
            self.access_order.remove(skill_id)
            self.access_order.append(skill_id)
            return self.cache[skill_id]
        return None

    def put(self, skill_id: str, skill_data: Dict):
        """æ·»åŠ åˆ°ç¼“å­˜"""
        if skill_id in self.cache:
            # å·²å­˜åœ¨ï¼Œæ›´æ–°è®¿é—®é¡ºåº
            self.access_order.remove(skill_id)
        elif len(self.cache) >= self.max_size:
            # ç¼“å­˜æ»¡ï¼Œç§»é™¤æœ€æ—§çš„
            oldest = self.access_order.pop(0)
            del self.cache[oldest]

        self.cache[skill_id] = skill_data
        self.access_order.append(skill_id)

    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.access_order.clear()


class CachedSkillLoader(SkillLoader):
    """å¸¦ç¼“å­˜çš„ Skill åŠ è½½å™¨"""

    def __init__(self, skills_dir: str = "knowledge_base/skills", cache_size: int = 20):
        super().__init__(skills_dir)
        self.cache = SkillCache(max_size=cache_size)

    def get_skill(self, skill_id: str) -> Optional[Dict]:
        """è·å– Skillï¼ˆä¼˜å…ˆä»ç¼“å­˜ï¼‰"""
        # æ£€æŸ¥ç¼“å­˜
        cached = self.cache.get(skill_id)
        if cached:
            return cached

        # ç¼“å­˜æœªå‘½ä¸­ï¼ŒåŠ è½½æ–‡ä»¶
        skill_data = super().get_skill(skill_id)

        if skill_data:
            self.cache.put(skill_id, skill_data)

        return skill_data
```

## æµ‹è¯•éªŒè¯

### 1. æ„å»ºç´¢å¼•æµ‹è¯•

```python
from skill_loader import SkillIndexBuilder

# æ„å»ºç´¢å¼•
builder = SkillIndexBuilder("knowledge_base/skills")
index = builder.build()

# ä¿å­˜ç´¢å¼•
builder.save()

# éªŒè¯
assert len(index) > 0
print(f"âœ… ç´¢å¼•åŒ…å« {len(index)} ä¸ª Skills")
```

### 2. Skill åŠ è½½æµ‹è¯•

```python
from skill_loader import SkillLoader

# åˆå§‹åŒ–åŠ è½½å™¨
loader = SkillLoader("knowledge_base/skills")

# åŠ è½½å•ä¸ª Skill
skill = loader.get_skill("sk-personal-tax")
assert skill is not None
assert "content" in skill
assert "metadata" in skill

print(f"âœ… Skill: {skill['metadata']['title']}")
print(f"âœ… å†…å®¹é•¿åº¦: {len(skill['content'])} å­—ç¬¦")

# æ‰¹é‡åŠ è½½
skills = loader.load_skills(["sk-personal-tax", "federal-personal-tax"])
assert len(skills) == 2

print(f"âœ… æ‰¹é‡åŠ è½½ {len(skills)} ä¸ª Skills")
```

### 3. æœç´¢æµ‹è¯•

```python
# æŒ‰é¢†åŸŸæœç´¢
provincial_skills = loader.search(domain="provincial_tax")
print(f"âœ… çœçº§ç¨åŠ¡ Skills: {provincial_skills}")

# æŒ‰è§¦å‘è¯æœç´¢
sk_skills = loader.search(triggers=["Saskatchewan", "è¨çœ"])
print(f"âœ… è¨çœç›¸å…³ Skills: {sk_skills}")

# è·å–ç›¸å…³ Skills
related = loader.get_related_skills("sk-personal-tax", priority="high")
print(f"âœ… ç›¸å…³ Skills: {related}")
```

### 4. æ€§èƒ½æµ‹è¯•

```python
import time

# æµ‹è¯•åŠ è½½é€Ÿåº¦
start = time.time()
skill = loader.get_skill("sk-personal-tax")
duration = (time.time() - start) * 1000

print(f"âœ… åŠ è½½è€—æ—¶: {duration:.2f} ms")
assert duration < 50  # åº”å°äº 50ms

# æµ‹è¯•æ‰¹é‡åŠ è½½
start = time.time()
skills = loader.load_skills([f"skill_{i}" for i in range(10)])
duration = time.time() - start

print(f"âœ… æ‰¹é‡åŠ è½½ 10 ä¸ª Skills è€—æ—¶: {duration:.2f} s")
```

### 5. ç¼“å­˜æµ‹è¯•

```python
from skill_loader import CachedSkillLoader

# ä½¿ç”¨ç¼“å­˜åŠ è½½å™¨
cached_loader = CachedSkillLoader("knowledge_base/skills", cache_size=10)

# é¦–æ¬¡åŠ è½½ï¼ˆæ…¢ï¼‰
start = time.time()
skill1 = cached_loader.get_skill("sk-personal-tax")
first_load_time = (time.time() - start) * 1000

# ç¬¬äºŒæ¬¡åŠ è½½ï¼ˆå¿«ï¼Œä»ç¼“å­˜ï¼‰
start = time.time()
skill2 = cached_loader.get_skill("sk-personal-tax")
cached_load_time = (time.time() - start) * 1000

print(f"âœ… é¦–æ¬¡åŠ è½½: {first_load_time:.2f} ms")
print(f"âœ… ç¼“å­˜åŠ è½½: {cached_load_time:.2f} ms")
assert cached_load_time < first_load_time
```

## æ³¨æ„äº‹é¡¹

**ç´¢å¼•æ›´æ–°ç­–ç•¥ï¼š**
- å¼€å‘é˜¶æ®µï¼šæ‰‹åŠ¨é‡å»ºç´¢å¼•ï¼ˆ`loader.rebuild_index()`ï¼‰
- ç”Ÿäº§ç¯å¢ƒï¼šå¯é€‰å®ç°æ–‡ä»¶ç›‘æ§è‡ªåŠ¨é‡å»º
- ç®€å•æ–¹æ¡ˆï¼šæ¯æ¬¡å¯åŠ¨æ—¶æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨

**å…ƒæ•°æ®éªŒè¯ï¼š**
- å¿…éœ€å­—æ®µï¼šskill_id, title, domain
- æ¨èå­—æ®µï¼štriggers, keywords, related_skills
- è‡ªåŠ¨è®¡ç®—ï¼šcontent_size_kb, estimated_tokens

**æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼š**
- ç´¢å¼•æ–‡ä»¶ç¼“å­˜åœ¨å†…å­˜ä¸­ï¼ˆå¯åŠ¨æ—¶åŠ è½½ä¸€æ¬¡ï¼‰
- Skill å†…å®¹æŒ‰éœ€åŠ è½½ï¼ˆä¸é¢„åŠ è½½å…¨éƒ¨ï¼‰
- å¯é€‰ä½¿ç”¨ LRU ç¼“å­˜çƒ­é—¨ Skills
- å•ä¸ªç´¢å¼•æ–‡ä»¶å¤§å°æ§åˆ¶åœ¨ < 1MB

**ä¸å‘é‡æ£€ç´¢æ–¹æ¡ˆå¯¹æ¯”ï¼š**

| ç‰¹æ€§ | è½»é‡çº§ç´¢å¼• | å‘é‡æ£€ç´¢ |
|------|----------|---------|
| å®ç°å¤æ‚åº¦ | â­ ç®€å• | â­â­â­ å¤æ‚ |
| ä¾èµ–é¡¹ | PyYAML | ChromaDB + Sentence Transformers |
| å†…å­˜å ç”¨ | < 10MB | > 500MB |
| ç´¢å¼•æ„å»º | < 1s | > 30sï¼ˆéœ€è¦ embeddingï¼‰ |
| é€‚ç”¨åœºæ™¯ | < 1000 Skills | æ— é™åˆ¶ |
| æ£€ç´¢æ–¹å¼ | å…ƒæ•°æ®åŒ¹é… | è¯­ä¹‰ç›¸ä¼¼åº¦ |

**ä»€ä¹ˆæ—¶å€™éœ€è¦å‡çº§åˆ°å‘é‡æ£€ç´¢ï¼š**
- Skills æ•°é‡ > 1000
- éœ€è¦è·¨æ–‡æ¡£çš„è¯­ä¹‰æ£€ç´¢
- éœ€è¦å¤„ç†æ¨¡ç³ŠæŸ¥è¯¢
- æ–‡æ¡£å†…å®¹é«˜åº¦ç›¸ä¼¼ï¼Œéš¾ä»¥ç”¨å…ƒæ•°æ®åŒºåˆ†

## ä¾èµ–å…³ç³»

**å‰ç½®ä»»åŠ¡ï¼š**
- ä»»åŠ¡09ï¼šMarkdown ç”Ÿæˆä¼˜åŒ–
- ä»»åŠ¡10ï¼šSkill æ¶æ„è®¾è®¡

**åç½®ä»»åŠ¡ï¼š**
- ä»»åŠ¡13ï¼šClaude è¾…åŠ©çš„ Skill è·¯ç”±å™¨
- ä»»åŠ¡14ï¼šSkill åŠ è½½å’Œä¸Šä¸‹æ–‡æ„å»ºå¼•æ“
