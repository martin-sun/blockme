# Multi-Stage Pipeline Architecture

**BeanFlow-CRA Skill ç”Ÿæˆç³»ç»ŸæŠ€æœ¯æ¶æ„æ–‡æ¡£**

æ·±å…¥äº†è§£ 6 é˜¶æ®µæµæ°´çº¿çš„è®¾è®¡ç†å¿µã€å®ç°ç»†èŠ‚å’Œæœ€ä½³å®è·µã€‚

---

## ğŸ“ æ¶æ„æ¦‚è¿°

### è®¾è®¡ç†å¿µ

**ä»å•ä½“åˆ°æ¨¡å—åŒ–**

```
ä¼ ç»Ÿå•ä½“æ¶æ„:
PDF â†’ [800+ è¡Œé»‘ç›’è„šæœ¬] â†’ Skill è¾“å‡º
âŒ ä¸­æ–­ = å…¨éƒ¨é‡æ¥
âŒ éš¾ä»¥è°ƒè¯•
âŒ æ— æ³•å¤ç”¨

å¤šé˜¶æ®µ Pipeline:
PDF â†’ Stage 1 â†’ Stage 2 â†’ Stage 3 â†’ Stage 4 â†’ Stage 5 â†’ Stage 6
      â†“ cache   â†“ cache   â†“ cache   â†“ cache   â†“ output
âœ… æ¯é˜¶æ®µç‹¬ç«‹è¿è¡Œ
âœ… ç»“æœå¯å¤ç”¨
âœ… æ–­ç‚¹ç»­ä¼ 
âœ… æ˜“äºè°ƒè¯•å’Œä¼˜åŒ–
```

### æ ¸å¿ƒåŸåˆ™

1. **å•ä¸€èŒè´£**: æ¯ä¸ª Stage åªåšä¸€ä»¶äº‹
2. **ç¼“å­˜ä¼˜å…ˆ**: æ‰€æœ‰ä¸­é—´ç»“æœæŒä¹…åŒ–
3. **å¯æ¢å¤æ€§**: å¤±è´¥åå¯ä»æ–­ç‚¹ç»§ç»­
4. **å¯è§‚æµ‹æ€§**: æ¸…æ™°çš„è¿›åº¦å’Œæ—¥å¿—
5. **è§£è€¦è®¾è®¡**: Stage ä¹‹é—´æ¾è€¦åˆ

---

## ğŸ”¬ Stage è¯¦è§£

### Stage 1: PDF æ–‡æœ¬æå–

**è„šæœ¬**: `stage1_extract_pdf.py`
**æ¨¡å—**: `app/document_processor/pdf_extractor.py`

#### æ ¸å¿ƒåŠŸèƒ½

```python
class PDFTextExtractor:
    """PDF æ–‡æœ¬æå–å™¨"""

    def extract(self, pdf_path: Path) -> ExtractionResult:
        """
        æå– PDF æ–‡æœ¬å†…å®¹

        Returns:
            ExtractionResult:
                - total_text: å®Œæ•´æ–‡æœ¬
                - pages: åˆ†é¡µç»“æœ
                - metadata: PDF å…ƒæ•°æ®
        """
```

#### æŠ€æœ¯ç»†èŠ‚

**PDF åº“**: PyMuPDF (fitz)
- âœ… é«˜æ€§èƒ½ï¼ˆC++ å®ç°ï¼‰
- âœ… ç²¾ç¡®çš„æ–‡æœ¬æå–
- âœ… ä¿ç•™å¸ƒå±€ä¿¡æ¯

**åˆ†é¡µæå–**:
```python
for page_num in range(total_pages):
    page = doc[page_num]
    text = page.get_text("text")

    # ä¿å­˜æ¯é¡µå…ƒæ•°æ®
    page_result = PageResult(
        page_number=page_num + 1,
        text=text,
        char_count=len(text),
        line_count=text.count('\n')
    )
```

**é¡µæ•°é™åˆ¶**:
- é»˜è®¤: å‰ 10 é¡µï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
- `--full`: æ‰€æœ‰é¡µé¢
- `--max-pages N`: è‡ªå®šä¹‰é¡µæ•°

#### ç¼“å­˜ç­–ç•¥

**Hash è®¡ç®—**:
```python
def hash_file(file_path: Path) -> str:
    """SHA256 hash çš„å‰ 16 ä½"""
    sha256 = hashlib.sha256()
    with open(file_path, 'rb') as f:
        while chunk := f.read(8192):
            sha256.update(chunk)
    return sha256.hexdigest()[:16]
```

**ç¼“å­˜é”®**: `extraction_{pdf_hash}.json`

**ç¼“å­˜å¤±æ•ˆæ¡ä»¶**:
1. PDF æ–‡ä»¶å†…å®¹æ”¹å˜ï¼ˆhash ä¸åŒï¼‰
2. `--max-pages` å‚æ•°æ”¹å˜
3. ä½¿ç”¨ `--force` å¼ºåˆ¶é‡æ–°æå–

#### æ€§èƒ½æŒ‡æ ‡

| PDF å¤§å° | é¡µæ•° | æå–æ—¶é—´ | ç¼“å­˜å¤§å° |
|---------|------|---------|---------|
| 870 KB | 151 | 30ç§’ - 2åˆ†é’Ÿ | ~1.5 MB |
| 200 KB | 30 | 10-20ç§’ | ~400 KB |
| 2 MB | 300 | 1-3åˆ†é’Ÿ | ~3 MB |

---

### Stage 2: å†…å®¹åˆ†ç±»

**è„šæœ¬**: `stage2_classify_content.py`
**æ¨¡å—**: `app/document_processor/content_classifier.py`

#### æ™ºèƒ½å¤šä¿¡å·åˆ†ç±»ç®—æ³•

**5 ä¸ªè¯„åˆ†ç»´åº¦**:

1. **å…³é”®è¯è¦†ç›–** (Keyword Coverage)
   ```python
   # è®¡ç®—åŒ¹é…çš„å…³é”®è¯æ•°é‡
   matched_keywords = [kw for kw in category_keywords if kw in content]
   score = len(matched_keywords) / len(category_keywords)
   ```

2. **ç»“æ„è´¨é‡** (Structure Quality)
   ```python
   # æ£€æµ‹æ–‡æ¡£ç»“æ„ç‰¹å¾
   has_headings = bool(re.search(r'^#{1,3}\s+', content, re.M))
   has_lists = bool(re.search(r'^\s*[-*]\s+', content, re.M))
   has_sections = detect_chapters(content) > 0
   score = (has_headings + has_lists + has_sections) / 3
   ```

3. **å†…å®¹æ·±åº¦** (Content Depth)
   ```python
   # è¯„ä¼°å†…å®¹æ·±åº¦
   char_count = len(content)
   paragraph_count = content.count('\n\n')
   avg_paragraph_length = char_count / max(paragraph_count, 1)
   score = min(avg_paragraph_length / 500, 1.0)
   ```

4. **ç‰¹å¼‚æ€§åˆ†æ•°** (Specificity Score)
   ```python
   # æ£€æµ‹ç‰¹å®šæœ¯è¯­
   specific_terms = ['T4', 'T2', 'RRSP', 'Line 150', etc.]
   found_terms = [t for t in specific_terms if t in content]
   score = len(found_terms) / len(specific_terms)
   ```

5. **å®Œæ•´æ€§åˆ†æ•°** (Completeness Score)
   ```python
   # è¯„ä¼°æ–‡æ¡£å®Œæ•´æ€§
   has_intro = 'introduction' in content.lower()
   has_examples = 'example' in content.lower()
   has_references = 'reference' in content.lower()
   score = (has_intro + has_examples + has_references) / 3
   ```

**æœ€ç»ˆåˆ†ç±»**:
```python
# åŠ æƒå¹³å‡
final_score = (
    keyword_coverage * 0.4 +
    structure_quality * 0.2 +
    content_depth * 0.15 +
    specificity_score * 0.15 +
    completeness_score * 0.1
)
```

#### åˆ†ç±»ç±»åˆ«

```python
class TaxCategory(str, Enum):
    PERSONAL_INCOME = "personal_income"
    EMPLOYMENT_INCOME = "employment_income"
    SELF_EMPLOYMENT = "self_employment"
    BUSINESS_INCOME = "business_income"
    CAPITAL_GAINS = "capital_gains"
    DEDUCTIONS = "deductions"
    CREDITS = "credits"
    RRSP = "rrsp"
    TFSA = "tfsa"
    GST_HST = "gst_hst"
    # ... æ›´å¤šç±»åˆ«
```

#### æ€§èƒ½

- **æ—¶é—´**: < 5 ç§’ï¼ˆçº¯è§„åˆ™åŒ¹é…ï¼Œæ—  LLM è°ƒç”¨ï¼‰
- **å‡†ç¡®ç‡**: ~85-90%ï¼ˆåŸºäºå…³é”®è¯å’Œç»“æ„ï¼‰

---

### Stage 3: å†…å®¹åˆ†å—

**è„šæœ¬**: `stage3_chunk_content.py`

#### æ™ºèƒ½ç« èŠ‚æ£€æµ‹

**3 ç§æ£€æµ‹æ–¹æ³•**ï¼ˆä¼˜å…ˆçº§é€’å‡ï¼‰:

1. **Markdown æ ‡é¢˜** (confidence: 1.0)
   ```python
   # æ£€æµ‹ # å’Œ ## æ ‡é¢˜
   pattern = r'^(#{1,2})\s+(.+)$'
   ```

2. **æ¨¡å¼åŒ¹é…** (confidence: 0.9)
   ```python
   patterns = [
       r'^Chapter\s+(\d+)[:\-\s](.+)$',
       r'^Part\s+(\d+)[:\-\s](.+)$',
       r'^Section\s+(\d+(?:\.\d+)?)[:\-\s](.+)$',
       r'^(\d+)\.\s+([A-Z][^\n]{5,50})$',  # "1. Introduction"
   ]
   ```

3. **å…¨å¤§å†™æ ‡é¢˜** (confidence: 0.7)
   ```python
   # æ£€æµ‹ INTRODUCTION, OVERVIEW ç­‰
   pattern = r'^([A-Z][A-Z\s]{5,50})$'
   # è¿‡æ»¤ false positives: PDF, CRA, GST, etc.
   ```

#### åˆ†å—ç­–ç•¥

**ç›®æ ‡**: æ¯ä¸ª chunk â‰¤ 300K å­—ç¬¦ï¼ˆClaude é™åˆ¶ï¼‰

**ç®—æ³•**:
```python
def split_by_chapters(content: str, max_chunk_size: int):
    chapters = detect_chapters(content)

    for chapter in chapters:
        chapter_content = extract_chapter(chapter)

        if len(chapter_content) > max_chunk_size:
            # ç« èŠ‚è¿‡å¤§ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
            sub_chunks = split_by_paragraphs(chapter_content)
            for sub in sub_chunks:
                yield ChunkResult(...)
        else:
            yield ChunkResult(...)
```

**æ®µè½çº§åˆ†å‰²**:
```python
def split_by_paragraphs(content: str, max_size: int):
    paragraphs = content.split('\n\n')

    current_chunk = ""
    for paragraph in paragraphs:
        if len(current_chunk) + len(paragraph) <= max_size:
            current_chunk += paragraph + "\n\n"
        else:
            yield current_chunk
            current_chunk = paragraph + "\n\n"
```

#### Chunk å…ƒæ•°æ®

```python
class ChunkResult:
    chunk_id: int           # 1-indexed
    title: str              # "Chapter 1: ..."
    slug: str               # "chapter-1-..."
    content: str            # ç« èŠ‚å†…å®¹
    char_count: int         # å­—ç¬¦æ•°
    chapter_num: int        # ç« èŠ‚ç¼–å·
```

#### å…¸å‹è¾“å‡º

**151 é¡µ PDF**:
- æ€»å­—ç¬¦æ•°: 721K
- Chunk æ•°é‡: 83
- å¹³å‡å¤§å°: ~8,700 chars/chunk
- æœ€å¤§ chunk: ~295K chars

---

### Stage 4: AI å¢å¼º â­

**è„šæœ¬**: `stage4_enhance_chunks.py`
**æœ€å¤æ‚çš„é˜¶æ®µ** - è€—æ—¶æœ€é•¿ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 

#### æ¶æ„è®¾è®¡

**æ ¸å¿ƒæ€æƒ³**: æ¸è¿›å¼å¤„ç† + ç«‹å³ä¿å­˜

```python
for chunk_id in chunks_to_process:
    # 1. å¢å¼ºå•ä¸ª chunk
    enhanced = enhance_single_chunk(chunk, provider)

    # 2. ç«‹å³ä¿å­˜ï¼ˆå…³é”®ï¼ï¼‰
    pipeline.save_enhanced_chunk(content_hash, chunk_id, enhanced)

    # 3. æ›´æ–°è¿›åº¦
    progress['completed_chunks'] = chunk_id
    pipeline.save_enhancement_progress(content_hash, progress)

    # 4. è®¡ç®— ETA
    eta = calculate_eta(completed, remaining, avg_time)
```

#### æ–­ç‚¹ç»­ä¼ å®ç°

**è¿›åº¦æ–‡ä»¶** `cache/enhanced_chunks_{hash}/progress.json`:

```json
{
  "total_chunks": 83,
  "completed_chunks": 40,
  "failed_chunks": [15, 23],
  "start_time": "2025-11-04T10:05:00",
  "last_update": "2025-11-04T13:30:00",
  "estimated_remaining": "180 minutes",
  "provider": "codex"
}
```

**ç»­ä¼ é€»è¾‘**:
```python
if args.resume:
    progress = load_progress(content_hash)
    completed = progress['completed_chunks']

    # ä»ä¸‹ä¸€ä¸ª chunk å¼€å§‹
    chunks_to_process = range(completed + 1, total_chunks + 1)

    print(f"Resuming from chunk {completed + 1}")
```

**å¤±è´¥é‡è¯•**:
```python
if args.retry_failed:
    progress = load_progress(content_hash)
    failed_chunks = progress['failed_chunks']

    # åªå¤„ç†å¤±è´¥çš„ chunks
    chunks_to_process = failed_chunks
```

#### LLM Provider æŠ½è±¡

**æ”¯æŒå¤šä¸ª LLM**:

```python
class LLMCLIProvider(ABC):
    @abstractmethod
    def build_command(self, prompt: str) -> List[str]:
        """æ„å»º CLI å‘½ä»¤"""

    @abstractmethod
    def parse_output(self, stdout: str, stderr: str) -> str:
        """è§£æ LLM è¾“å‡º"""

    @abstractmethod
    def get_timeout(self, content_length: int) -> int:
        """è®¡ç®—è¶…æ—¶æ—¶é—´"""
```

**å®ç°**:
- `ClaudeCLIProvider`: Claude Code CLI
- `GeminiCLIProvider`: Gemini CLI
- `CodexCLIProvider`: OpenAI Codex

#### Prompt è®¾è®¡

```python
prompt = f"""Please optimize this CRA tax content for the '{category}' category.

Requirements:
1. Keep all factual information accurate and complete
2. Add practical examples where appropriate
3. Improve clarity and structure
4. Use professional Canadian tax terminology
5. Format as clean Markdown with proper headers (##, ###)
6. Make it actionable for developers building tax applications

IMPORTANT: Output ONLY the enhanced Markdown content, nothing else.

Content to enhance:
{chunk}

Enhanced content (Markdown only):"""
```

#### è¶…æ—¶ç­–ç•¥

**åŠ¨æ€è¶…æ—¶**:
```python
def get_timeout(content_length: int) -> int:
    MIN_TIMEOUT = 240  # 4 åˆ†é’ŸåŸºç¡€
    TIMEOUT_PER_1K_CHARS = 5  # æ¯ 1K å­—ç¬¦ +5 ç§’

    return max(MIN_TIMEOUT, content_length // 1000 * TIMEOUT_PER_1K_CHARS)
```

**ç¤ºä¾‹**:
- 10K chars â†’ 240ç§’ (4åˆ†é’Ÿ)
- 100K chars â†’ 500ç§’ (8.3åˆ†é’Ÿ)
- 300K chars â†’ 1500ç§’ (25åˆ†é’Ÿ)

#### é”™è¯¯å¤„ç†

```python
try:
    enhanced = enhance_single_chunk(...)
    save_chunk(enhanced)
    progress['completed_chunks'] += 1

except subprocess.TimeoutExpired:
    logger.error(f"Chunk {id} timeout")
    progress['failed_chunks'].append(id)

except Exception as e:
    logger.error(f"Chunk {id} failed: {e}")
    progress['failed_chunks'].append(id)

finally:
    save_progress(progress)
```

#### æ€§èƒ½ä¼˜åŒ–

**1. å¹¶è¡Œå¤„ç†** (æœªå®ç°ï¼Œè§„åˆ’ä¸­):
```python
# ä½¿ç”¨ asyncio å¹¶è¡Œå¤„ç†å¤šä¸ª chunks
async def enhance_chunks_parallel(chunks, max_concurrent=4):
    semaphore = asyncio.Semaphore(max_concurrent)
    tasks = [enhance_chunk_async(c, semaphore) for c in chunks]
    return await asyncio.gather(*tasks)
```

**2. æ‰¹é‡å¤„ç†** (æœªå®ç°ï¼Œè§„åˆ’ä¸­):
```python
# å°†å¤šä¸ªå° chunks åˆå¹¶æˆä¸€ä¸ªè¯·æ±‚
def batch_small_chunks(chunks, max_size=300_000):
    batch = []
    current_size = 0

    for chunk in chunks:
        if current_size + len(chunk) <= max_size:
            batch.append(chunk)
            current_size += len(chunk)
        else:
            yield batch
            batch = [chunk]
            current_size = len(chunk)
```

---

### Stage 5: Skill ç”Ÿæˆ

**è„šæœ¬**: `stage5_generate_skill.py`
**æ¨¡å—**: `app/document_processor/skill_generator.py`

#### ç›®å½•ç»“æ„ç”Ÿæˆ

```python
def save_skill_directory(
    skill_id: str,
    raw_text: str,
    reference_chunks: List[dict],
    metadata: SkillMetadata
) -> Path:
    """
    ç”Ÿæˆ Skill ç›®å½•ç»“æ„

    skill_id/
    â”œâ”€â”€ SKILL.md               # ç´¢å¼•æ–‡ä»¶
    â”œâ”€â”€ references/
    â”‚   â”œâ”€â”€ index.md           # å¯¼èˆªç´¢å¼•
    â”‚   â”œâ”€â”€ chunk-1-slug.md    # å¢å¼ºå†…å®¹
    â”‚   â”œâ”€â”€ chunk-2-slug.md
    â”‚   â””â”€â”€ ...
    â””â”€â”€ raw/
        â””â”€â”€ full-extract.txt   # åŸå§‹æ–‡æœ¬
    """
```

#### SKILL.md åŸºç¡€ç‰ˆæœ¬

**ç”Ÿæˆé€»è¾‘**:
```python
def _create_skill_index(
    path: Path,
    metadata: SkillMetadata,
    reference_files: List[dict],
    raw_text_size: int
):
    content = f"""---
id: {metadata.id}
title: {metadata.title}
tags: {metadata.tags}
description: {metadata.description}
category: {metadata.category}
---

# {metadata.title}

## ğŸ“– When to Use This Skill

{generate_use_cases(metadata.category)}

## ğŸ“š Reference Documentation

{generate_toc(reference_files)}

## ğŸ“Š Document Statistics

- Total chapters: {len(reference_files)}
- Raw text size: {raw_text_size:,} chars
- Category: {metadata.category}
    """
```

**è´¨é‡**: 3/10ï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰

---

### Stage 6: SKILL.md å¢å¼ºï¼ˆå¯é€‰ï¼‰

**è„šæœ¬**: `enhance_skill.py`
**æ¨¡å—**: `app/document_processor/skill_enhancer.py`

è¯¦è§ [SKILL_ENHANCEMENT.md](SKILL_ENHANCEMENT.md)

**æå‡**: 3/10 â†’ 9/10

---

## ğŸ’¾ ç¼“å­˜ç³»ç»Ÿè¯¦è§£

### Cache Manager

**æ ¸å¿ƒç±»**: `app/document_processor/pipeline_manager.py`

```python
class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: Path = None):
        self.cache_dir = cache_dir or Path("backend/cache")

    def hash_file(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶ SHA256 hashï¼ˆå‰16ä½ï¼‰"""

    def save_cache(self, stage: PipelineStage, hash: str, data: dict):
        """ä¿å­˜ç¼“å­˜"""

    def load_cache(self, stage: PipelineStage, hash: str) -> Optional[dict]:
        """åŠ è½½ç¼“å­˜"""

    def cache_exists(self, stage: PipelineStage, hash: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
```

### ç¼“å­˜é”®è®¾è®¡

**ç»Ÿä¸€ Hash**: æ‰€æœ‰ stage ä½¿ç”¨ç›¸åŒçš„ PDF hash ä½œä¸ºæ ‡è¯†

```
PDF â†’ hash: abc123

cache/
â”œâ”€â”€ extraction_abc123.json
â”œâ”€â”€ classification_abc123.json
â”œâ”€â”€ chunks_abc123.json
â””â”€â”€ enhanced_chunks_abc123/
```

**ä¼˜åŠ¿**:
- âœ… åŒä¸€ PDF çš„æ‰€æœ‰ stages å…³è”
- âœ… æ˜“äºæ‰¹é‡æ¸…ç†
- âœ… è‡ªåŠ¨å¤±æ•ˆï¼ˆPDF æ”¹å˜ â†’ æ–° hashï¼‰

### ç¼“å­˜æ ¼å¼

**æ ‡å‡†æ ¼å¼**:
```json
{
  "stage": "extraction",
  "content_hash": "abc123",
  "timestamp": "2025-11-04T10:00:00",
  "metadata": {
    "pdf_path": "../mvp/pdf/t4012-24e.pdf",
    "total_pages": 151
  },
  "data": {
    // Stage-specific data
  }
}
```

### ç¼“å­˜ç®¡ç†

**æ¸…ç†ç­–ç•¥**:
```python
# æŒ‰æ—¶é—´æ¸…ç†
cache_mgr.clean_cache(older_than_days=7)

# æŒ‰ hash æ¸…ç†
cache_mgr.clean_cache(content_hash="abc123")
```

**æŸ¥çœ‹ç¼“å­˜**:
```python
# åˆ—å‡ºæ‰€æœ‰ç¼“å­˜çš„ PDFs
pdfs = cache_mgr.list_cached_pdfs()
for pdf in pdfs:
    print(f"{pdf['content_hash']}: {pdf['pdf_path']}")
```

---

## ğŸ”„ Pipeline Manager

### ç¼–æ’é€»è¾‘

**æ ¸å¿ƒç±»**: `PipelineManager`

```python
class PipelineManager:
    """Pipeline ç¼–æ’å™¨"""

    def get_stage_status(self, content_hash: str) -> Dict[PipelineStage, bool]:
        """è·å–æ‰€æœ‰ stage çš„å®ŒæˆçŠ¶æ€"""

    def get_enhancement_progress(self, content_hash: str) -> Optional[dict]:
        """è·å– enhancement è¿›åº¦"""

    def save_enhancement_progress(self, content_hash: str, progress: dict):
        """ä¿å­˜ enhancement è¿›åº¦"""

    def load_enhanced_chunks(self, content_hash: str) -> List[dict]:
        """åŠ è½½æ‰€æœ‰å¢å¼ºåçš„ chunks"""
```

### Stage ä¾èµ–å…³ç³»

```
Stage 1: PDF Extraction
  â†“ (needs: PDF file)

Stage 2: Content Classification
  â†“ (needs: extraction_<hash>.json)

Stage 3: Content Chunking
  â†“ (needs: extraction_<hash>.json)

Stage 4: AI Enhancement
  â†“ (needs: chunks_<hash>.json + classification_<hash>.json)

Stage 5: Skill Generation
  â†“ (needs: enhanced_chunks_<hash>/ + classification_<hash>.json)

Stage 6: SKILL.md Enhancement
  (needs: skill directory)
```

### ç»Ÿä¸€å…¥å£

**`generate_skill.py`**: Pipeline ç¼–æ’å™¨

```python
def main():
    # 1. è®¡ç®— PDF hash
    pdf_hash = cache_mgr.hash_file(args.pdf)

    # 2. æ£€æŸ¥ç¼“å­˜çŠ¶æ€
    status = pipeline.get_stage_status(pdf_hash)

    # 3. ä¾æ¬¡è¿è¡Œå„ stageï¼ˆè·³è¿‡å·²ç¼“å­˜ï¼‰
    run_stage_script('stage1_extract_pdf.py', ...)
    run_stage_script('stage2_classify_content.py', ...)
    run_stage_script('stage3_chunk_content.py', ...)
    run_stage_script('stage4_enhance_chunks.py', ...)
    run_stage_script('stage5_generate_skill.py', ...)

    # 4. å¯é€‰: å¢å¼º SKILL.md
    if args.enhance_skill:
        run_stage_script('enhance_skill.py', ...)
```

---

## ğŸ“Š æ€§èƒ½åˆ†æ

### ç“¶é¢ˆè¯†åˆ«

**Stage 4 å  99%+ æ—¶é—´**:

```
Stage 1-3: < 3 åˆ†é’Ÿ (< 1%)
Stage 4:   415-664 åˆ†é’Ÿ (>99%)
Stage 5-6: < 10 åˆ†é’Ÿ (< 1%)
```

### ä¼˜åŒ–æ–¹å‘

**1. å¹¶è¡ŒåŒ–** (æœ€æœ‰æ•ˆ):
- 4 ä¸ªå¹¶å‘ â†’ ç†è®ºåŠ é€Ÿ 4x
- å®é™…: 100-150 åˆ†é’Ÿï¼ˆ151é¡µï¼‰

**2. æ›´å¤§çš„ Chunk**:
- Gemini: 1.5M tokens â†’ 17 chunks (vs 83)
- åŠ é€Ÿ: ~5x

**3. æ›´å¿«çš„ LLM**:
- Gemini 2.0 Flash: å“åº”æ›´å¿«
- ä½†å¯èƒ½ç‰ºç‰²è´¨é‡

**4. æ‰¹é‡è¯·æ±‚**:
- å°†å¤šä¸ªå° chunks åˆå¹¶æˆä¸€ä¸ªè¯·æ±‚
- å‡å°‘è¯·æ±‚æ•°

---

## ğŸ› è°ƒè¯•æŒ‡å—

### æ—¥å¿—åˆ†æ

**Stage è¾“å‡ºæ ¼å¼**:
```
============================================================
Stage N: [Description]
============================================================
[Progress information]
[Status updates]
âœ… Success / âŒ Failed
```

### å¸¸è§é—®é¢˜å®šä½

**1. æŸ¥çœ‹ç¼“å­˜çŠ¶æ€**:
```bash
ls -lh backend/cache/
cat backend/cache/extraction_*.json | jq .
```

**2. æŸ¥çœ‹è¿›åº¦**:
```bash
cat backend/cache/enhanced_chunks_*/progress.json | jq .
```

**3. æ£€æŸ¥å¤±è´¥ chunk**:
```bash
jq '.failed_chunks' backend/cache/enhanced_chunks_*/progress.json
```

**4. é‡æ–°è¿è¡Œç‰¹å®š stage**:
```bash
# å¼ºåˆ¶é‡æ–°æå–
uv run python stage1_extract_pdf.py --pdf FILE.pdf --force

# é‡è¯•å¤±è´¥çš„ chunks
uv run python stage4_enhance_chunks.py --chunks-id abc123 --retry-failed
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### å¼€å‘è¿­ä»£

**1. å¿«é€Ÿæµ‹è¯•**:
```bash
# åªå¤„ç† 10 é¡µ
uv run python generate_skill.py --pdf FILE.pdf --glm-api
```

**2. è°ƒè¯• Pipeline**:
```bash
# æ‰‹åŠ¨è¿è¡Œå„ stageï¼Œè§‚å¯Ÿè¾“å‡º
uv run python stage1_extract_pdf.py --pdf FILE.pdf
uv run python stage2_classify_content.py --extraction-id abc123
```

**3. æµ‹è¯• AI å¢å¼º**:
```bash
# åªå¤„ç† 30 é¡µï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
uv run python generate_skill.py --pdf FILE.pdf --max-pages 30 --local-codex
```

### ç”Ÿäº§éƒ¨ç½²

**1. æ‰¹é‡å¤„ç†**:
```bash
for pdf in pdfs/*.pdf; do
    uv run python generate_skill.py --pdf "$pdf" --local-codex --full
done
```

**2. ç›‘æ§è¿›åº¦**:
```bash
# å®šæœŸæ£€æŸ¥è¿›åº¦
watch -n 60 'cat backend/cache/enhanced_chunks_*/progress.json | jq .'
```

**3. å®šæœŸæ¸…ç†ç¼“å­˜**:
```bash
# æ¯å‘¨æ¸…ç†
python -c "from app.document_processor.pipeline_manager import CacheManager; \
           CacheManager().clean_cache(older_than_days=7)"
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **[Backend README](../README.md)** - å¿«é€Ÿå¼€å§‹æŒ‡å—
- **[SKILL Enhancement](SKILL_ENHANCEMENT.md)** - SKILL.md å¢å¼ºåŠŸèƒ½
- **[Cache README](../cache/README.md)** - ç¼“å­˜æ ¼å¼è¯´æ˜

---

**ä½œè€…**: BeanFlow Team
**ç‰ˆæœ¬**: 2.0
**æ›´æ–°**: 2025-11-04
